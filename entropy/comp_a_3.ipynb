{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "4ed11f55-000a-426b-9faf-d12c80f2994e",
      "metadata": {},
      "source": [
       "# Character-level entropy: reproducing results from Shannon (1951)\n",
       "In this notebook, we will explore the computation of various flavors of entropy using natural language as a test-bed.  We will perform these explorations at the level of characters (although an interesting analysis can also be done at the word level).\n",
       "\n",
       "## Part 1: The baseline\n",
       "As a matter of comparison, and in order to get a sense of the magnitude of the entropy numbers that we expect to be dealing with, let's compute the entropy of the alphabet under the assumption that each character is independent and uniformly distributed.  As our alphabet, let's take the 26 letters (A-Z), the digits (0-9), and the space character.  Ignore case and punctuation.  Derive the simplest expression that you can for the entropy of this distribution for a single character.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "5353d815-9273-4667-b154-54e999d8acc4",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "913df782-55cb-4dbb-be88-9b78012ee843",
      "metadata": {},
      "source": [
       "## Part 2: Unigram entropy\n",
       "Next, we will consider the character-level entropy of natural english.  In words, our task is as follows: given some character drawn from a corpus of English text, how many yes or no questions do I need to ask on average to figure out the character, assuming questions are asked optimally?  To compute the entropy, we need a text on which to base our estimates of frequency.  Python's [Natural Language Toolkit (NLTK)](https://www.nltk.org) is a nice tool for this purpose.  We'll look at a few different text corpi (the others being the Book of Genesis and a spanish-language corpus_, starting with the 'Brown dataset', which is around 1M words long, and was compiled at Brown University from a large number of newspaper articles.  First, install nltk via pip (or whatever), then we can acquire the corpus as  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b6b1b9-b948-4781-9b23-a0b76cd66c7d",
      "metadata": {},
      "outputs": [],
      "source": [
       "import nltk\n",
       "nltk.download('brown')\n",
       "nltk.download('genesis')\n",
       "nltk.download('cess_esp')\n",
       "\n",
       "from nltk.corpus import brown,genesis,cess_esp\n",
       "print(brown.sents())"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "8cc39e04-ea28-4ae2-bb83-92949ab982cf",
      "metadata": {},
      "source": [
       "For the sake of simplicity, we want to use the same alphabet as in the previous section - here is a function which strips out all punctuation and special characters, casts to lowercase, and then puts all of the words together as one big string:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6cf2852-a589-469f-85cb-d7f337e52e54",
      "metadata": {},
      "outputs": [],
      "source": [
       "import string\n",
       "import re\n",
       "import itertools\n",
       "\n",
       "def process_nltk_corpus(corpus):\n",
       "    text = list(itertools.chain.from_iterable(corpus.sents()))\n",
       "    char_string = re.sub('[^A-Za-z0-9 ]+', '', re.sub(r\"\\s+\", \" \", ' '.join(text).translate(str.maketrans('', '', string.punctuation))).strip().lower())\n",
       "    return char_string\n",
       "\n",
       "text_brown = process_nltk_corpus(brown)\n",
       "print(text_brown[:500])"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "806ae3a3-c3f2-4cf0-848a-9da33e7c51e7",
      "metadata": {},
      "source": [
       "**Compute the empirical distribution over characters** (e.g. $p_a = \\frac{n_a}{N}$, with $n_a$ the number of $a$'s that appear, and $N$ the total number of characters), then **compute the entropy of the distribution**.   "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "82160a7f-3152-45c5-90cd-63bbbde862a1",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "89867497-7056-423b-b5ab-5dadb91a2908",
      "metadata": {},
      "source": [
       "Claude Shannon also computed this number (for a different corpus) and published it in a manuscript called [Prediction and Entropy of Printed English](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf) in 1951.  His estimate can be found in the table on p. 54, under the column header $F_1$ (the outcome of the previous section would be under column header $F_a$.  Compare your result to his and comment on potential reasons for any deviations.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f506f546-3f48-44a2-b6d6-286ef1021c28",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "49cdd591-bdcd-4ec8-ba0b-ea43d02c141f",
      "metadata": {},
      "source": [
       "## Part 3: Joint entropy over bigrams\n",
       "Obviously the distribution over the usage of characters in English is less random than uniform - but ultimately language is determined by the relationships between letters rather than the letters in isolation.  As such, let's explore a bigram model, which is to say, we'll be dealing with the distribution over two-letter pairs:\n",
       "$$P(X_1,X_2).$$\n",
       "First, if we were to assume that $X_1$ and $X_2$ were independent of one another (of course they aren't in reality), what would be the joint entropy of $X_1$ and $X_2$? *Note that you shouldn't have to do much to get this - the answer is an immediate consequence of your response to Part 2.* Again, this serves as an upper bound on the true joint entropy."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10126f9-8f1f-4698-9216-259acf706f81",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "8e130f8e-dd7e-4ff1-9065-55155b5afcb0",
      "metadata": {},
      "source": [
       "Next, let's compute the actual joint entropy given the text corpus.  To compute this, you will need to first determine the empirical joint distribution over all possible bigrams (of which there are $37^2$).  How you organize these is up to you.  With this distribution in hand, compute the joint entropy $H(X_1,X_2)$.  How does this compare to the baseline assuming independence?"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e254cde-adb5-47fb-9096-147421faec24",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "1a594005-489c-496a-a8e4-3aa2c7673dff",
      "metadata": {},
      "source": [
       "## Part 4: Conditional entropy over bigrams\n",
       "The next natural question to ask is: how predictable is the the next word ($X_2$) given knowledge the previous one ($X_1$)?  This is precisely the answer given by the conditional entropy\n",
       "$$ H(X_2|X_1) = \\sum_{x_1\\in\\mathcal{X}} P(X=x_1) H(X_2 | X_1=x_1).$$\n",
       "**Develop a method to compute the conditional entropy**.  Compare your result to Shannon, whose calculation is shown in his table under the column heading $F_2$.  \n",
       "\n",
       "*Note that you can also check to ensure that your code is functioning properly by using the identity\n",
       "$$\n",
       "H(X_1,X_2) = H(X_2|X_1) + H(X_1).\n",
       "$$\n",
       "You computed both of the quantities on the right hand side previously, so it should be trivial to ensure that this equality holds.*"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "4513031c-090c-4bac-bbda-669053e7f3e2",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "c86a7b11-729f-4aba-b3cd-1086b9f3cca7",
      "metadata": {},
      "source": [
       "## Part 5: Mutual Information\n",
       "Exactly how many bits of information does knowing the first letter provide me about the second letter?  Compute the mutual information in two ways: first, using the definition based on the Kullback-Leibler divergence:\n",
       "$$I(X_1;X_2) = \\sum_{x_1\\in\\mathcal{X}} \\sum_{x_2\\in\\mathcal{X}} P(x_1,x_2) \\lg \\frac{P(x_1,x_2)}{P(x_1)P(x_2)} $$ "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d69d2dab-a2f2-4d0f-88f3-7c0cd34f6808",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "017a46d5-b869-4feb-97f7-5cab3645c2e1",
      "metadata": {},
      "source": [
       "and second using the identity \n",
       "$$ I(X_1;X_2) = H(X_2) - H(X_2 | X_1)$$ "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "e675c533-0aef-4ad5-a0a3-83b68103ae6d",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "b562fc66-0159-48f6-9320-ce490ce2b10e",
      "metadata": {},
      "source": [
       "## Part 6: Kullback-Leibler Divergence\n",
       "Recall that there is a close relationship between the entropy of a random variable and the most efficient way in which that random variable can be encoded as a binary sequence.  The Kullback-Leibler divergence\n",
       "$$\n",
       "D(P(X) || Q(X)) = \\sum_{x\\in\\mathcal{X}} P(x) \\lg \\frac{P(x)}{Q(x)}\n",
       "$$\n",
       "measures the inefficiency (measured in extra bits) of encoding a distribution $P(X)$ with a distribution designed for $Q(X)$.  We have already seen the KL-divergence applied to answering the question \"how much efficiency do we lose by assuming independence\", but we can use this more generally.  In particular, please answer the question: \"how many extra bits do I lose by encoding Spanish characters using a code optimized for English?\"  Stated alternatively, **what is the KL-divergence between $P(X)$ - defined as the unigram distribution computed from the English corpus that we've already been working with - and $Q(X)$ - defined as the unigram distribution computed from a Spanish corpus** (please find a Spanish corpus in the following code snippet).  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcddbe2-b91c-43c8-bcbc-e4e7a04771d7",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "500bdc34-862a-4908-8e04-c40bf4eacf83",
      "metadata": {},
      "source": [
       "Comment on your result, in particular whether it says anything on the universality of language.  Do you think your result would change if you considered joint distributions rather than univariate ones?"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "b92f1d32-4e68-4ffa-aaed-0e33197a1b2a",
      "metadata": {},
      "source": [
       "# Huffman Coding\n",
       "### (no need to work on this part quite yet - we will get there soon)\n",
       "We are already familiar with Huffman codes: they are the binary sequences of answers that optimally encode a random variable (optimal with respect to minimizing expected number of questions), and as such are deeply tied to entropy.  **Create a method that builds the Huffman coding tree given a sequence of characters.**  *You will want to build some simple test cases to ensure correct functionality*.  Once you are sure that your method is working, construct the Huffman coding tree for the Brown corpus described above.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41f8d81-0175-4aa9-9f70-ee68412752d5",
      "metadata": {},
      "outputs": [],
      "source": [
       "def build_huffman_tree(text):\n",
       "    \"\"\"\n",
       "    Build a Huffman tree from the given text\n",
       "    \n",
       "    Args:\n",
       "        text: Input string\n",
       "        \n",
       "    Returns:\n",
       "        root: Root node of Huffman tree\n",
       "    \"\"\""
      ]
     },
     {
      "cell_type": "markdown",
      "id": "e0e2900b-abe8-4bc7-9938-187500515d69",
      "metadata": {},
      "source": [
       "With this tree in hand, **encode the Brown corpus.**    "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04d253d-24a1-4de9-8f7e-75cdb459e4b4",
      "metadata": {},
      "outputs": [],
      "source": [
       "def huffman_encoding(text):\n",
       "    \"\"\"\n",
       "    Perform Huffman coding on the input text\n",
       "    \n",
       "    Args:\n",
       "        text: Input string containing lowercase letters, digits, or spaces\n",
       "        \n",
       "    Returns:\n",
       "        encoded_text: Binary string of the encoded text\n",
       "        huffman_tree: Root node of the Huffman tree (for decoding)\n",
       "        codes: Dictionary mapping characters to their Huffman codes\n",
       "    \"\"\"\n",
       "    \n",
       "    return encoded_text, huffman_tree, codes"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "cd64731a-8720-4893-bdc2-6395912731e3",
      "metadata": {},
      "source": [
       "**Report the compression factor** (the ratio of bits required to represent the unencoded and encoded versions of the corpus).  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ffe6e0b-f70f-44a6-a9aa-666ea3a55a08",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "1942f80d-98a2-43f7-8368-55540595aa76",
      "metadata": {},
      "source": [
       "**Report the average number of bits used to encode each symbol in the corpus**.  Compare this to the entropy that you calculated previously.  How does your Huffman coding scheme compare to the entropy (which provides the theoretical lower limit on this quantity)? "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcda6952-8135-40a5-8774-1ea61c937db7",
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }