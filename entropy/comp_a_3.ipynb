{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed11f55-000a-426b-9faf-d12c80f2994e",
   "metadata": {},
   "source": [
    "# Character-level entropy: reproducing results from Shannon (1951)\n",
    "In this notebook, we will explore the computation of various flavors of entropy using natural language as a test-bed.  We will perform these explorations at the level of characters (although an interesting analysis can also be done at the word level).\n",
    "\n",
    "## Part 1: The baseline\n",
    "As a matter of comparison, and in order to get a sense of the magnitude of the entropy numbers that we expect to be dealing with, let's compute the entropy of the alphabet under the assumption that each character is independent and uniformly distributed.  As our alphabet, let's take the 26 letters (A-Z), the digits (0-9), and the space character.  Ignore case and punctuation.  Derive the simplest expression that you can for the entropy of this distribution for a single character.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7205ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353d815-9273-4667-b154-54e999d8acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphanumeric_chars = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ', 'a', 'b', 'c', 'd', 'e', 'f',\n",
    "                     'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
    "                     'w', 'x', 'y', 'z']\n",
    "total_chars = len(alphanumeric_chars)\n",
    "entropy = np.log2(total_chars)\n",
    "print(f\"Entropy of alphanumeric characters: {entropy:.2f} bits per character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913df782-55cb-4dbb-be88-9b78012ee843",
   "metadata": {},
   "source": [
    "## Part 2: Unigram entropy\n",
    "Next, we will consider the character-level entropy of natural english.  In words, our task is as follows: given some character drawn from a corpus of English text, how many yes or no questions do I need to ask on average to figure out the character, assuming questions are asked optimally?  To compute the entropy, we need a text on which to base our estimates of frequency.  Python's [Natural Language Toolkit (NLTK)](https://www.nltk.org) is a nice tool for this purpose.  We'll look at a few different text corpi (the others being the Book of Genesis and a spanish-language corpus_, starting with the 'Brown dataset', which is around 1M words long, and was compiled at Brown University from a large number of newspaper articles.  First, install nltk via pip (or whatever), then we can acquire the corpus as  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6b1b9-b948-4781-9b23-a0b76cd66c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('genesis')\n",
    "nltk.download('cess_esp')\n",
    "\n",
    "from nltk.corpus import brown,genesis,cess_esp\n",
    "print(brown.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc39e04-ea28-4ae2-bb83-92949ab982cf",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we want to use the same alphabet as in the previous section - here is a function which strips out all punctuation and special characters, casts to lowercase, and then puts all of the words together as one big string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf2852-a589-469f-85cb-d7f337e52e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "def process_nltk_corpus(corpus):\n",
    "    text = list(itertools.chain.from_iterable(corpus.sents()))\n",
    "    char_string = re.sub('[^A-Za-z0-9 ]+', '', re.sub(r\"\\s+\", \" \", ' '.join(text).translate(str.maketrans('', '', string.punctuation))).strip().lower())\n",
    "    return char_string\n",
    "\n",
    "text_brown = process_nltk_corpus(brown)\n",
    "print(text_brown[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_brown.count('zb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ae3a3-c3f2-4cf0-848a-9da33e7c51e7",
   "metadata": {},
   "source": [
    "**Compute the empirical distribution over characters** (e.g. $p_a = \\frac{n_a}{N}$, with $n_a$ the number of $a$'s that appear, and $N$ the total number of characters), then **compute the entropy of the distribution**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82160a7f-3152-45c5-90cd-63bbbde862a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_distribution(text):\n",
    "    # count the occurrences of each character\n",
    "    char_counts = {char: text.count(char) for char in alphanumeric_chars}\n",
    "    \n",
    "    # calc total number of characters\n",
    "    total_count = sum(char_counts.values())\n",
    "    \n",
    "    # calc empirical distribution\n",
    "    empirical_distribution = {char: count / total_count for char, count in char_counts.items()}\n",
    "    \n",
    "    return empirical_distribution\n",
    "\n",
    "def compute_entropy(empirical_distribution):\n",
    "    # calc entropy\n",
    "    entropy = -sum(p * np.log2(p) for p in empirical_distribution.values() if p > 0) # sum over non-zero probabilities\n",
    "    return entropy\n",
    "    \n",
    "def plot_empirical_distribution(empirical_distribution):\n",
    "    # plots the empirical distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(empirical_distribution.keys(), empirical_distribution.values())\n",
    "    plt.xlabel('Characters')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Empirical Distribution of Characters in Brown Corpus')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "empirical_distribution_brown = compute_empirical_distribution(text_brown)\n",
    "entropy_brown = compute_entropy(empirical_distribution_brown)\n",
    "print(f\"Entropy of Brown Corpus: {entropy_brown:.2f} bits per character\")\n",
    "# plot\n",
    "plot_empirical_distribution(empirical_distribution_brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89867497-7056-423b-b5ab-5dadb91a2908",
   "metadata": {},
   "source": [
    "Claude Shannon also computed this number (for a different corpus) and published it in a manuscript called [Prediction and Entropy of Printed English](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf) in 1951.  His estimate can be found in the table on p. 54, under the column header $F_1$ (the outcome of the previous section would be under column header $F_a$.  Compare your result to his and comment on potential reasons for any deviations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746d215",
   "metadata": {},
   "source": [
    "---\n",
    "My answer is the same: 4.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99362af9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdd591-bdcd-4ec8-ba0b-ea43d02c141f",
   "metadata": {},
   "source": [
    "## Part 3: Joint entropy over bigrams\n",
    "Obviously the distribution over the usage of characters in English is less random than uniform - but ultimately language is determined by the relationships between letters rather than the letters in isolation.  As such, let's explore a bigram model, which is to say, we'll be dealing with the distribution over two-letter pairs:\n",
    "$$P(X_1,X_2).$$\n",
    "First, if we were to assume that $X_1$ and $X_2$ were independent of one another (of course they aren't in reality), what would be the joint entropy of $X_1$ and $X_2$? *Note that you shouldn't have to do much to get this - the answer is an immediate consequence of your response to Part 2.* Again, this serves as an upper bound on the true joint entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3d507",
   "metadata": {},
   "source": [
    "---\n",
    "Answer: The joint entropy of $X_1$ and $X_2$ is the sum of the entropies of the two variabless, which is equal to 8.28 bits.  This is an upper bound on the true joint entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515565d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e130f8e-dd7e-4ff1-9065-55155b5afcb0",
   "metadata": {},
   "source": [
    "Next, let's compute the actual joint entropy given the text corpus.  To compute this, you will need to first determine the empirical joint distribution over all possible bigrams (of which there are $37^2$).  How you organize these is up to you.  With this distribution in hand, compute the joint entropy $H(X_1,X_2)$.  How does this compare to the baseline assuming independence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e254cde-adb5-47fb-9096-147421faec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_joint_distribution(text):\n",
    "    # count the occurrences of each bigram\n",
    "    bigram_counts = {}\n",
    "    for i in range(len(text) - 1):\n",
    "        bigram = text[i:i + 2]\n",
    "        if len(bigram) == 2:\n",
    "            if bigram not in bigram_counts:\n",
    "                bigram_counts[bigram] = 0\n",
    "            bigram_counts[bigram] += 1\n",
    "    \n",
    "    # calc total number of bigrams\n",
    "    total_count = sum(bigram_counts.values())\n",
    "    \n",
    "    # calc empirical joint distribution\n",
    "    empirical_joint_distribution = {bigram: count / total_count for bigram, count in bigram_counts.items()}\n",
    "    \n",
    "    return empirical_joint_distribution, bigram_counts\n",
    "def compute_joint_entropy(empirical_joint_distribution):\n",
    "    # calc joint entropy (same as before)\n",
    "    joint_entropy = -sum(p * np.log2(p) for p in empirical_joint_distribution.values() if p > 0) # sum over non-zero probabilities\n",
    "    return joint_entropy\n",
    "def plot_empirical_joint_distribution(empirical_joint_distribution):\n",
    "    # plots the empirical joint distribution\n",
    "    # replace spaces with '_'\n",
    "    temp_empirical_joint_distribution = {bigram.replace(' ', '_'): prob for bigram, prob in empirical_joint_distribution.items()}\n",
    "    # plot the top N bigrams only\n",
    "    n_top = 30\n",
    "    sorted_bigrams = sorted(temp_empirical_joint_distribution.items(), key=lambda x: x[1], reverse=True)[:n_top]\n",
    "    bigrams, probabilities = zip(*sorted_bigrams)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(bigrams, probabilities)\n",
    "    plt.xlabel('Bigrams')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Empirical Joint Distribution of Bigrams in Brown Corpus')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "empirical_joint_distribution_brown, bigrams = compute_empirical_joint_distribution(text_brown)\n",
    "joint_entropy_brown = compute_joint_entropy(empirical_joint_distribution_brown)\n",
    "print(f\"Joint Entropy of Brown Corpus: {joint_entropy_brown:.2f} bits per character\")\n",
    "# plot\n",
    "plot_empirical_joint_distribution(empirical_joint_distribution_brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a594005-489c-496a-a8e4-3aa2c7673dff",
   "metadata": {},
   "source": [
    "## Part 4: Conditional entropy over bigrams\n",
    "The next natural question to ask is: how predictable is the the next word ($X_2$) given knowledge the previous one ($X_1$)?  This is precisely the answer given by the conditional entropy\n",
    "$$ H(X_2|X_1) = \\sum_{x_1\\in\\mathcal{X}} P(X=x_1) H(X_2 | X_1=x_1).$$\n",
    "**Develop a method to compute the conditional entropy**.  Compare your result to Shannon, whose calculation is shown in his table under the column heading $F_2$.  \n",
    "\n",
    "*Note that you can also check to ensure that your code is functioning properly by using the identity\n",
    "$$\n",
    "H(X_1,X_2) = H(X_2|X_1) + H(X_1).\n",
    "$$\n",
    "You computed both of the quantities on the right hand side previously, so it should be trivial to ensure that this equality holds.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513031c-090c-4bac-bbda-669053e7f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_entropy(empirical_joint_distribution, empirical_distribution):\n",
    "    # calc conditional entropy\n",
    "    conditional_entropy = {}\n",
    "    for bigram, p_xy in empirical_joint_distribution.items():\n",
    "        x1 = bigram[0]\n",
    "        p_x1 = empirical_distribution[x1]\n",
    "        if p_x1 > 0:\n",
    "            # calculate the entropy for the bigram\n",
    "            conditional_entropy[bigram] = -p_xy * np.log2(p_xy / p_x1)\n",
    "    \n",
    "    # sum over all bigrams\n",
    "    conditional_entropy_sum = sum(conditional_entropy.values())\n",
    "    \n",
    "    return conditional_entropy_sum\n",
    "\n",
    "conditional_entropy_brown = compute_conditional_entropy(empirical_joint_distribution_brown, empirical_distribution_brown)\n",
    "# sanity check\n",
    "print(f\"Conditional Entropy: {conditional_entropy_brown:.2f} bits per character\")\n",
    "print(f\"Joint Entropy: {joint_entropy_brown:.2f} bits per character\")\n",
    "print(f\"Single Character Entropy: {entropy_brown:.2f} bits per character\")\n",
    "print(f\"Conditional Entropy = Joint Entropy - Single Character Entropy: {joint_entropy_brown - entropy_brown:.2f} bits per character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a7b11-729f-4aba-b3cd-1086b9f3cca7",
   "metadata": {},
   "source": [
    "## Part 5: Mutual Information\n",
    "Exactly how many bits of information does knowing the first letter provide me about the second letter?  Compute the mutual information in two ways: first, using the definition based on the Kullback-Leibler divergence:\n",
    "$$I(X_1;X_2) = \\sum_{x_1\\in\\mathcal{X}} \\sum_{x_2\\in\\mathcal{X}} P(x_1,x_2) \\lg \\frac{P(x_1,x_2)}{P(x_1)P(x_2)} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d2dab-a2f2-4d0f-88f3-7c0cd34f6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first\n",
    "def compute_mutual_information(empirical_joint_distribution, empirical_distribution):\n",
    "    # calc mutual information\n",
    "    mutual_information = 0\n",
    "    # formula:\n",
    "    # where H(X) = entropy of X, H(Y) = entropy of Y, H(X,Y) = joint entropy of X and Y\n",
    "    # mutual information = sum(p_xy * log2(p_xy / (p_x1 * p_x2)))\n",
    "    # where p_xy = joint probability of X and Y, p_x1 = marginal probability of X, p_x2 = marginal probability of Y\n",
    "    # p_xy = empirical_joint_distribution[bigram]\n",
    "    # p_x1 = empirical_distribution[x1]\n",
    "    # p_x2 = empirical_distribution[x2]\n",
    "    # mutual_information = sum(p_xy * log2(p_xy / (p_x1 * p_x2)))\n",
    "    # where bigram = (x1, x2)\n",
    "    for bigram, p_xy in empirical_joint_distribution.items():\n",
    "        x1 = bigram[0]\n",
    "        x2 = bigram[1]\n",
    "        p_x1 = empirical_distribution[x1]\n",
    "        p_x2 = empirical_distribution[x2]\n",
    "        if p_x1 > 0 and p_x2 > 0:\n",
    "            mutual_information += p_xy * np.log2(p_xy / (p_x1 * p_x2))\n",
    "    \n",
    "    return mutual_information\n",
    "mutual_information_brown = compute_mutual_information(empirical_joint_distribution_brown, empirical_distribution_brown)\n",
    "print(f\"Mutual Information of Brown Corpus (KL defined): {mutual_information_brown:.2f} bits per character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a46d5-b869-4feb-97f7-5cab3645c2e1",
   "metadata": {},
   "source": [
    "and second using the identity \n",
    "$$ I(X_1;X_2) = H(X_2) - H(X_2 | X_1)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675c533-0aef-4ad5-a0a3-83b68103ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second\n",
    "def compute_mutual_info_identity(empirical_joint_distribution, empirical_distribution):\n",
    "    # calc mutual information\n",
    "    mutual_information = 0\n",
    "    # formula:\n",
    "    # I(X_1;X_2) = H(X_2) - H(X_2 | X_1)\n",
    "    # where H(X_1) = entropy of X_1, H(X_2) = entropy of X_2, H(X_2 | X_1) = conditional entropy of X_2 given X_1\n",
    "    mutual_information = compute_entropy(empirical_distribution) - compute_conditional_entropy(empirical_joint_distribution, empirical_distribution)\n",
    "    \n",
    "    return mutual_information\n",
    "mutual_information_brown_identity = compute_mutual_info_identity(empirical_joint_distribution_brown, empirical_distribution_brown)\n",
    "print(f\"Mutual Information of Brown Corpus (Identity): {mutual_information_brown_identity:.2f} bits per character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562fc66-0159-48f6-9320-ce490ce2b10e",
   "metadata": {},
   "source": [
    "## Part 6: Kullback-Leibler Divergence\n",
    "Recall that there is a close relationship between the entropy of a random variable and the most efficient way in which that random variable can be encoded as a binary sequence.  The Kullback-Leibler divergence\n",
    "$$\n",
    "D(P(X) || Q(X)) = \\sum_{x\\in\\mathcal{X}} P(x) \\lg \\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "measures the inefficiency (measured in extra bits) of encoding a distribution $P(X)$ with a distribution designed for $Q(X)$.  We have already seen the KL-divergence applied to answering the question \"how much efficiency do we lose by assuming independence\", but we can use this more generally.  In particular, please answer the question: \"how many extra bits do I lose by encoding Spanish characters using a code optimized for English?\"  Stated alternatively, **what is the KL-divergence between $P(X)$ - defined as the unigram distribution computed from the English corpus that we've already been working with - and $Q(X)$ - defined as the unigram distribution computed from a Spanish corpus** (please find a Spanish corpus in the following code snippet).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcddbe2-b91c-43c8-bcbc-e4e7a04771d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_corpus = process_nltk_corpus(cess_esp)\n",
    "\n",
    "# measure KL divergence\n",
    "def compute_kl_divergence(empirical_distribution_1, empirical_distribution_2):\n",
    "    kl_divergence = 0\n",
    "    for char in empirical_distribution_1.keys():\n",
    "        if empirical_distribution_1[char] > 0 and empirical_distribution_2[char] > 0:\n",
    "            kl_divergence += empirical_distribution_1[char] * np.log2(empirical_distribution_1[char] / empirical_distribution_2[char])\n",
    "    return kl_divergence\n",
    "empirical_distribution_spanish = compute_empirical_distribution(spanish_corpus)\n",
    "kl_divergence_brown_spanish = compute_kl_divergence(empirical_distribution_brown, empirical_distribution_spanish)\n",
    "print(f\"KL Divergence between Brown Corpus and Spanish Corpus: {kl_divergence_brown_spanish:.2f} bits per character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500bdc34-862a-4908-8e04-c40bf4eacf83",
   "metadata": {},
   "source": [
    "Comment on your result, in particular whether it says anything on the universality of language.  Do you think your result would change if you considered joint distributions rather than univariate ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f1d32-4e68-4ffa-aaed-0e33197a1b2a",
   "metadata": {},
   "source": [
    "# Huffman Coding\n",
    "### (no need to work on this part quite yet - we will get there soon)\n",
    "We are already familiar with Huffman codes: they are the binary sequences of answers that optimally encode a random variable (optimal with respect to minimizing expected number of questions), and as such are deeply tied to entropy.  **Create a method that builds the Huffman coding tree given a sequence of characters.**  *You will want to build some simple test cases to ensure correct functionality*.  Once you are sure that your method is working, construct the Huffman coding tree for the Brown corpus described above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b1919d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counts_brown = {char: text_brown.count(char) for char in alphanumeric_chars}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ccec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counts_brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "02e163c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "heap_test = [[weight, [char, \"\"]] for char, weight in char_counts_brown.items()] # creates a list of lists (psuedo nodes) a la heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heap_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85cac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heap_test.sort()\n",
    "heap_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = heap_test.pop(0)\n",
    "hi = heap_test.pop(0)\n",
    "# assign binary codes to the characters\n",
    "# lo[1:] is the list of characters in the left subtree \n",
    "# hi[1:] is the list of characters in the right subtree\n",
    "# lo[0] is the weight of the left subtree, hi[0] is the weight of the right subtree\n",
    "print(\"lo: \", lo)\n",
    "print(\"hi: \", hi)\n",
    "for pair in lo[1:]:\n",
    "    pair[1] = '0' + pair[1] # add '0' to the binary code of the characters in the left subtree\n",
    "for pair in hi[1:]: \n",
    "    pair[1] = '1' + pair[1] # add '1' to the binary code of the characters in the right subtree\n",
    "print(\"lo after: \", lo)\n",
    "print(\"hi after: \", hi)\n",
    "# the new node is the sum of the two smallest nodes\n",
    "new_node = [lo[0] + hi[0]] + lo[1:] + hi[1:]\n",
    "print(\"new_node: \", new_node)\n",
    "heap_test.append(new_node) # add the new node to the heap, with the key being the sum of the two smallest nodes, and the value being the list of characters in the left and right subtrees\n",
    "heap_test.sort() # sort again to maintain the lowest weight at the top\n",
    "heap_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a41f8d81-0175-4aa9-9f70-ee68412752d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_huffman_tree(text):\n",
    "\n",
    "    # count the occurrences of each character\n",
    "    char_counts = {char: text.count(char) for char in alphanumeric_chars}\n",
    "    \n",
    "    # create a priority queue (min-heap) of nodes from scratch\n",
    "    # each node is a list of two elements: [weight, [char, code]]\n",
    "    # where weight is the frequency of the character, char is the character, and code is the binary code\n",
    "    # the code is initially empty, it is filled in as we iterate through the pairs\n",
    "    heap = [[weight, [char, \"\"]] for char, weight in char_counts.items()]\n",
    "    heap.sort() # this give us a min-heap with the smallest weight (probability) at the top\n",
    "    \n",
    "    while len(heap) > 1: # while there are more than one node in the heap\n",
    "        # pop the two smallest nodes from the heap\n",
    "        # the 'smallest' means the lowest weight (lowest probability)\n",
    "        lo = heap.pop(0)\n",
    "        hi = heap.pop(0)\n",
    "        # assign binary codes to the characters\n",
    "        # lo[1:] is the list of characters in the left subtree - t\n",
    "        # hi[1:] is the list of characters in the right subtree\n",
    "        # lo[0] is the weight of the left subtree\n",
    "        for pair in lo[1:]:\n",
    "            pair[1] = '0' + pair[1] # add '0' to the binary code of the characters in the left subtree\n",
    "        for pair in hi[1:]: \n",
    "            pair[1] = '1' + pair[1] # add '1' to the binary code of the characters in the right subtree\n",
    "        # the new node is the sum of the two smallest nodes\n",
    "        # add the new node to the heap, with the key being the sum of the two smallest nodes (aka the probability of the new node), and the value being the list of characters in the left and right subtrees (binary codes)\n",
    "        heap.append([lo[0] + hi[0]] + lo[1:] + hi[1:]) \n",
    "        heap.sort()\n",
    "        # when the heap has only one node left, we have built the Huffman tree\n",
    "        # the remaining node is the root of the tree and contains the entire tree\n",
    "    \n",
    "    return heap[0][1:] # return the list of characters and their binary codes (root of the tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0a21559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "huffman_tree = build_huffman_tree(text_brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f08b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "huffman_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2900b-abe8-4bc7-9938-187500515d69",
   "metadata": {},
   "source": [
    "With this tree in hand, **encode the Brown corpus.**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b04d253d-24a1-4de9-8f7e-75cdb459e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_encoding(text):\n",
    "\n",
    "    # build the Huffman tree\n",
    "    huffman_tree = build_huffman_tree(text)\n",
    "\n",
    "    # create a dictionary to store the codes\n",
    "    codes = {}\n",
    "    for pair in huffman_tree:\n",
    "        # by iterating over the pairs in the Huffman tree, we can get the character and its corresponding binary code\n",
    "        # pair is a list of two elements: [char, code]\n",
    "        char = pair[0]\n",
    "        code = pair[1]\n",
    "        codes[char] = code\n",
    "    # encode the text using a string join operation with list comprehension (this is like a map function)\n",
    "    encoded_text = ''.join(codes[char] for char in text) # for each character in the text, get its code from the dictionary and join them together\n",
    "    # return the encoded text, Huffman tree, and codes\n",
    "    return encoded_text, huffman_tree, codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64731a-8720-4893-bdc2-6395912731e3",
   "metadata": {},
   "source": [
    "**Report the compression factor** (the ratio of bits required to represent the unencoded and encoded versions of the corpus).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0ffe6e0b-f70f-44a6-a9aa-666ea3a55a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression factor: 1.92\n"
     ]
    }
   ],
   "source": [
    "def compute_compression_factor(text, encoded_text):\n",
    "\n",
    "    # num bits original text\n",
    "    original_bits = len(text) * 8  # assuming 8 bits per character\n",
    "    # num bits in the encoded text\n",
    "    encoded_bits = len(encoded_text)\n",
    "    # calc the compression factor\n",
    "    compression_factor = original_bits / encoded_bits\n",
    "    return compression_factor\n",
    "# encode the text\n",
    "encoded_text, huffman_tree, codes = huffman_encoding(text_brown)\n",
    "# compute the compression factor\n",
    "compression_factor = compute_compression_factor(text_brown, encoded_text)\n",
    "print(f\"Compression factor: {compression_factor:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0fc6413b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': '0000',\n",
       " 'u': '00010',\n",
       " 'c': '00011',\n",
       " 'e': '001',\n",
       " 'r': '0100',\n",
       " 's': '0101',\n",
       " 'n': '0110',\n",
       " 'k': '0111000',\n",
       " 'v': '0111001',\n",
       " 'y': '011101',\n",
       " 'w': '011110',\n",
       " 'g': '011111',\n",
       " 'i': '1000',\n",
       " 'o': '1001',\n",
       " 'a': '1010',\n",
       " 'd': '10110',\n",
       " 'l': '10111',\n",
       " 't': '1100',\n",
       " 'p': '110100',\n",
       " 'f': '110101',\n",
       " 'm': '110110',\n",
       " 'x': '110111000',\n",
       " 'q': '1101110010',\n",
       " '1': '1101110011',\n",
       " '2': '11011101000',\n",
       " '6': '110111010010',\n",
       " '4': '110111010011',\n",
       " 'j': '1101110101',\n",
       " '3': '110111011000',\n",
       " '9': '110111011001',\n",
       " '0': '11011101101',\n",
       " '5': '110111011100',\n",
       " '7': '1101110111010',\n",
       " '8': '1101110111011',\n",
       " 'z': '11011101111',\n",
       " 'b': '1101111',\n",
       " ' ': '111'}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9b015c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11000000001111110101000101011111001001011011100011100100010011011000111011110111110100101001101011011111011101010001001000111011110101101010001011011111010101001000101101010011101111101001101111000011001110010010101110010000111111010110010001001011011110011101011111010110010111101001101100101001011110100001000110010110110011111010001001000110110101001000111011110011011100100011110010001001011011111010001001001101100001000011001101101110110100111100101110011000101100010110000110011111100000010101'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text[:100]  # show the first 100 characters of the encoded text\n",
    "# decode the text\n",
    "\n",
    "def huffman_decoding(encoded_text, codes):\n",
    "    \n",
    "    # create a reverse mapping of codes to characters\n",
    "    reverse_codes = {code: char for char, code in codes.items()}\n",
    "    # decode the text\n",
    "    decoded_text = \"\"\n",
    "    current_code = \"\"\n",
    "    for bit in encoded_text: # iterate over the bits in the encoded text\n",
    "        # append the bit to the current code\n",
    "        current_code += bit\n",
    "        if current_code in reverse_codes: # if the current code is in the reverse mapping\n",
    "            # append the character to the decoded text\n",
    "            decoded_text += reverse_codes[current_code]\n",
    "            current_code = \"\"\n",
    "    return decoded_text\n",
    "\n",
    "# decode the text\n",
    "decoded_text = huffman_decoding(encoded_text, codes)\n",
    "# check if the decoded text is the same as the original text\n",
    "if decoded_text == text_brown:\n",
    "    print(\"Decoded text matches the original text\")\n",
    "else:\n",
    "    print(\"Decoded text does not match the original text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942f80d-98a2-43f7-8368-55540595aa76",
   "metadata": {},
   "source": [
    "**Report the average number of bits used to encode each symbol in the corpus**.  Compare this to the entropy that you calculated previously.  How does your Huffman coding scheme compare to the entropy (which provides the theoretical lower limit on this quantity)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "fcda6952-8135-40a5-8774-1ea61c937db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of bits per symbol: 4.18\n",
      "Entropy of Brown Corpus: 4.14 bits per character\n",
      "Joint Entropy of Brown Corpus: 7.53 bits per character\n",
      "Conditional Entropy of Brown Corpus: 3.39 bits per character\n"
     ]
    }
   ],
   "source": [
    "def compute_average_bits_per_symbol(encoded_text, text):\n",
    "\n",
    "    # calculate the number of bits in the encoded text\n",
    "    encoded_bits = len(encoded_text)\n",
    "    # calculate the number of symbols in the original text\n",
    "    num_symbols = len(text)\n",
    "    # calculate the average number of bits per symbol\n",
    "    average_bits_per_symbol = encoded_bits / num_symbols\n",
    "    return average_bits_per_symbol\n",
    "# compute the average number of bits per symbol\n",
    "average_bits_per_symbol = compute_average_bits_per_symbol(encoded_text, text_brown)\n",
    "print(f\"Average number of bits per symbol: {average_bits_per_symbol:.2f}\")\n",
    "# compare to entropy\n",
    "print(f\"Entropy of Brown Corpus: {entropy_brown:.2f} bits per character\")\n",
    "# compare to joint entropy\n",
    "print(f\"Joint Entropy of Brown Corpus: {joint_entropy_brown:.2f} bits per character\")\n",
    "# compare to conditional entropy\n",
    "print(f\"Conditional Entropy of Brown Corpus: {conditional_entropy_brown:.2f} bits per character\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aispace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
