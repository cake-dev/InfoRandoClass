{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "eaf23da1-4fe9-48b7-ad7a-df0012e11789",
      "metadata": {},
      "source": [
       "# Randomized Linear Algebra\n",
       "![title](machine_learning.png)\n",
       "\n",
       "Linear algebra forms the bedrock of many machine learning algorithms, providing the essential tools for data manipulation and analysis.  For example, linear regression involves the solutin of a linear system of equations to determine optimal coefficients defining the relationship between variables.  In clustering algorithms, especially those relying on distance metrics like k-means, linear algebra facilitates the calculation of distances between data points, enabling the grouping of similar items.  Principal component analysis (PCA), a dimensionality reduction technique, heavily relies on eigenvalue and eigenvector decompositions of covariance matrices, all core concepts of linear algebra.  Critically, linear algebra provides the framework for representing and manipulating data as vectors and matrices.  \n",
       "\n",
       "However, oftentimes we work with datasets that are sufficiently large such that the computational complexity of typical linear algebraic operations such as matrix multiplication, inversion, or decomposition are too expensive.  In such circumstances, randomization provides us tools such that we can trade a (sometimes much) faster runtime for a (surprisingly little) reduced accuracy.  In the following notebook, we will explore such randomized methods as applied to the cases of linear regression, k-means, and principal component analysis.  \n",
       "\n",
       "## Linear regression with radial basis functions\n",
       "Imagine that we are interested in creating a topographic map based on a collection of measured locations.  Let's define the spatial coordinates $\\hat{\\mathbf{x}}$ at which the elevation $\\hat{y}$ is measured.  Such a collection of measurements might appear as follows:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "485d3eff-f976-4fde-854b-38ae47cd5298",
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "np.random.seed(0)\n",
       "n_data = 2**11\n",
       "x = np.random.rand(n_data,2)\n",
       "\n",
       "D2 = (x[:,0] - x[:,0].reshape(-1,1))**2 + (x[:,1] - x[:,1].reshape(-1,1))**2\n",
       "\n",
       "sigma = 0.3\n",
       "l = 0.1\n",
       "K = np.exp(-D2/l**2)\n",
       "C = np.linalg.cholesky(K + np.eye(n_data)*1e-9)\n",
       "\n",
       "y = C @ np.random.randn(n_data) + sigma*np.random.randn(n_data)\n",
       "\n",
       "plt.scatter(*x.T,c=y,vmin=-2,vmax=2)\n",
       "plt.axis('equal')\n",
       "plt.colorbar()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "81f4535a-4b84-40e1-8798-8f3e76ac7836",
      "metadata": {},
      "source": [
       "Now that we have a dataset, we would like to create an interpolant - a function $y(x)$ that can accept arbitrary $x$ and predict an associated elevation, but that produces sensible behavior with respect to the observations - mainly that close to the observations they agree, and away from observations the solutions exhibit smooth behavior.  One way to create such a function is through radial basis function regression.  Such a method presupposes that the desired function is a sum of radial basis functions\n",
       "$$\n",
       "y(\\mathbf{x}) = \\sum_{k=1}^m \\phi_k(\\mathbf{x}) w_k,\n",
       "$$\n",
       "where \n",
       "$$\n",
       "\\phi_k(\\mathbf{x}) = \\exp\\left(-\\frac{(\\mathbf{x} - \\mu_k)^2}{l_k^2}\\right), \n",
       "$$\n",
       "is a radial basis function (here a so-called squared exponential), each with a different mean and a length scale that we select a priori.  As an example, a radial basis function with mean $\\mu_k=[0.5,0.75]$ and with length scale $l=0.1$, might look like this when evaluated over the test set:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6a2fb1-e8ef-49ac-be21-d7d8c947572c",
      "metadata": {},
      "outputs": [],
      "source": [
       "mu = np.array([0.5,0.75])\n",
       "l = 0.1\n",
       "phi_k = np.exp(-((x - mu)**2).sum(axis=1)/l**2)\n",
       "plt.scatter(*x.T,c=phi_k)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "5a29754e-bce3-464e-94af-912d30a071fb",
      "metadata": {},
      "source": [
       "Our solution is just a linear combination of such basis functions.  We suppose that the means are equally spaced in both spatial dimensions, and that they overlap a bit.  We collect the basis functions into a matrix $\\Phi(\\mathbf{x}) \\in \\mathbb{R}^{n \\times m}$ where each column corresponds to a different basis function, and where the rows correspond to a pointwise evaluation of that function.  We can then write our prediction $y = \\Phi(\\mathbf{x}) \\mathbf{w}$."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "648acffb-a854-4a44-991a-b348dfba358b",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create a grid of means\n",
       "x_ = np.linspace(0,1,11)\n",
       "y_ = x_\n",
       "X,Y = np.meshgrid(x_,y_)\n",
       "\n",
       "means = np.c_[X.ravel(),Y.ravel()]\n",
       "l = 0.1\n",
       "\n",
       "# Build the $\\Phi$ matrix\n",
       "Phi = np.column_stack([np.exp(-((x - m)**2).sum(axis=1)/l**2) for m in means])\n",
       "m = Phi.shape[1]"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "6c39c270-147f-41a1-bc0d-a3a86a658962",
      "metadata": {},
      "source": [
       "Just to get a sense for what functions are possible under this model, we could try it out with some random coefficients and evaluated over the training data points we saw before:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c6eee7-ca72-4cde-8230-3b14ea9e9280",
      "metadata": {},
      "outputs": [],
      "source": [
       "fig,axs = plt.subplots(nrows=2,ncols=2)\n",
       "axs = axs.ravel()\n",
       "for ax in axs:\n",
       "    y_random = Phi @ np.random.randn(Phi.shape[1])\n",
       "    ax.scatter(*x.T,c=y_random)\n",
       "    ax.axis('equal')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "01dea234-7220-40c1-8797-254701d5a21e",
      "metadata": {},
      "source": [
       "Note that this is just one way of constructing an interpolant, and it's subject to many arbitrary choices!  Nonetheless, based on the random functions that we generated above, we hope that we ought to be able to reasonably approximate our dataset.   \n",
       "\n",
       "Of course, *a priori* we don't know what the weight matrix $\\mathbf{w}$ is - we will need to constrain it with data.  In particular, we'd like to minimize the squared $L_2$ norm between the model's predictions and the observations, measured by the cost function\n",
       "$$\n",
       "\\mathcal{L}(\\mathbf{w}) = (\\Phi(\\mathbf{x}) \\mathbf{w} - \\hat{y})^T\\Sigma^{-1}(\\Phi(\\mathbf{x}) \\mathbf{w} - \\hat{y})\n",
       "$$\n",
       "where $\\hat{y}$ are the function values observed at $\\mathbf{x}$, and the matrix $\\Sigma^{1}$ is - in this case - a diagonal matrix containing the variance in the observations.  In addition, we'd like to constrain the problem so that the values $\\mathbf{w}$ don't stray too far from zero, so we add the regularization\n",
       "$$\n",
       "\\mathcal{P}(\\mathbf{w}) = \\mathbf{w}^T \\Sigma_0^{-1} \\mathbf{w},\n",
       "$$\n",
       "where $\\Sigma_0$ is a (usually diagonal) regularization matrix.  The minimization problem is then\n",
       "$$\n",
       "\\underset{\\mathbf{w}}{\\operatorname{argmin}} \\; \\mathcal{L}(\\mathbf{w}) + \\mathcal{P}(\\mathbf{w})\n",
       "$$\n",
       "We immediately recognize this as a typical least squares problem, which has as a solution\n",
       "$$\n",
       "\\left(\\Phi^T \\Sigma^{-1} \\Phi + \\Sigma_0^{-1}\\right) \\mathbf{w} = \\Phi^T \\Sigma^{-1} \\hat{y}.\n",
       "$$\n",
       "We can solve this immediately to get our estimate of $\\mathbf{w}$"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f870f534-8876-40ac-94cc-4f69bdb56afd",
      "metadata": {},
      "outputs": [],
      "source": [
       "class LinearRegression:\n",
       "    def __init__(self,sigma2=1.0,gamma=1.0):\n",
       "        self.sigma2 = sigma2\n",
       "\n",
       "    def fit(self,X,y):\n",
       "        self.X = X\n",
       "        self.y = y\n",
       "        self.w = np.linalg.solve(X.T/self.sigma2 @ X + np.eye(X.shape[1]),X.T/self.sigma2 @ y)\n",
       "\n",
       "    def predict(self,X):\n",
       "        return X @ self.w\n",
       "        \n",
       "model = LinearRegression(sigma2=sigma**2)\n",
       "model.fit(Phi,y)\n",
       "y_exact = model.predict(Phi)\n",
       "w_exact = model.w\n",
       "\n",
       "fig,axs = plt.subplots(ncols=2)\n",
       "fig.set_size_inches(8,4)\n",
       "axs[0].scatter(*x.T,c=y_exact,vmin=-3,vmax=3)\n",
       "axs[0].axis('equal')\n",
       "axs[0].set_title('Model')\n",
       "axs[1].scatter(*x.T,c=y,vmin=-3,vmax=3)\n",
       "axs[1].axis('equal')\n",
       "axs[1].set_title('Observation')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "b6cc6bbd-3600-421e-9f8e-0d4720ae8596",
      "metadata": {},
      "source": [
       "This approximates the data well (with the exception of the noise - our model is much smoother, which is probably a good thing).  How expensive was this?  The matrix $\\Phi^T \\Sigma^{-1} \\Phi$ is $m \\times m$, so solving this linear system via direct means is $m^3$.  However, the constructing that matrix costs $n m^2$ to perform the matrix multiplication.  The addition of these two terms yields the expense.\n",
       "\n",
       "The $n$ term can be prohibitive for some very large problems.  However, if we could reduce the dimensionality of the matrix, we could get a faster algorithm.  For instance, consider solving instead the minimization problem with\n",
       "$$\n",
       "\\mathcal{L}(\\mathbf{w}) = (S \\Phi(\\mathbf{x}) \\mathbf{w} - S \\hat{y})^T\\Sigma^{-1}(S \\Phi(\\mathbf{x}) \\mathbf{w} - S \\hat{y})\n",
       "$$\n",
       "for some $S\\in\\mathbb{R}^{d \\times n}$ (Note that to ensure that the relative weighting of the data misfit versus the prior remains the same, we have to scale $S$ appropriately).  If we had the matrix $S \\Phi$, then the resulting complexity would only be $\\mathcal{O}(dm^2 + m^3)$ (assuming $d$ is $\\mathcal{O}(m)$).  Does this work?  Let's find out.  \n",
       "\n",
       "### Subsampling\n",
       "First, what should we use as $S$?  We could try subsampling, such that \n",
       "$$\n",
       "S = \\sqrt{\\frac{n}{d}} I[s],\n",
       "$$\n",
       "where $s$ is a set of random indices over $n$.\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e1ec5f-3953-408f-87c6-ba2339583d02",
      "metadata": {},
      "outputs": [],
      "source": [
       "d = 200\n",
       "np.random.seed(0)\n",
       "indices = np.random.choice(n_data,d,replace=False)\n",
       "S = np.sqrt(n_data/d)*np.eye(n_data)[indices]\n",
       "\n",
       "model.fit(S @ Phi,S @ y)\n",
       "y_subsample = model.predict(Phi)\n",
       "\n",
       "fig,axs = plt.subplots(ncols=3)\n",
       "fig.set_size_inches(12,4)\n",
       "axs[0].scatter(*x.T,c=y_subsample,vmin=-3,vmax=3)\n",
       "axs[0].axis('equal')\n",
       "axs[0].set_title('Approx')\n",
       "axs[1].scatter(*x.T,c=y_exact,vmin=-3,vmax=3)\n",
       "axs[1].axis('equal')\n",
       "axs[1].set_title('Exact')\n",
       "axs[2].scatter(*x.T,c=y_subsample - y_exact,vmin=-1,vmax=1,cmap=plt.cm.seismic)\n",
       "axs[2].axis('equal')\n",
       "axs[2].set_title('Diff')\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "9388605f-e723-4f09-834d-eff5bb1ea72d",
      "metadata": {},
      "source": [
       "Note that as implemented, this isn't any more efficient, because the matrix product $S \\Phi$ takes $\\mathcal{O}(d n m)$ time to compute (which is similar to the $\\mathcal{O}(nm^2)$ of the exact case).  However, we note that we can take the matrix product $S \\Phi$ in time faster than $d n m$ by just extracting the appropriate rows from $\\Phi$.   This feels obvious of course, but it speaks to a more general pattern for gaining efficiency, namely choosing $S$ so that evaluating the product $S \\Phi$ is cheaper.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5e39b2-5a10-401b-b43a-92865926c216",
      "metadata": {},
      "outputs": [],
      "source": [
       "class SubsampleSketch:\n",
       "    def __init__(self,d):\n",
       "        self.d = d\n",
       "\n",
       "    def sketch(self,A,b=None,seed=None):\n",
       "        if seed is not None:\n",
       "            np.random.seed(seed)\n",
       "        n,m = A.shape\n",
       "        indices = np.random.choice(n,d,replace=False)  # Choose d random indices from n\n",
       "        scale = np.sqrt(n/d)\n",
       "        SA = scale*A[indices]\n",
       "        if b is not None:\n",
       "            Sb = scale*b[indices]\n",
       "            return SA,Sb\n",
       "        else:\n",
       "            return SA\n",
       "\n",
       "d = 200\n",
       "sketch_subsample = SubsampleSketch(d)\n",
       "\n",
       "model.fit(*sketch_subsample.sketch(Phi,b=y,seed=0))\n",
       "y_subsample = model.predict(Phi)\n",
       "\n",
       "fig,axs = plt.subplots(ncols=3)\n",
       "fig.set_size_inches(12,4)\n",
       "axs[0].scatter(*x.T,c=y_subsample,vmin=-3,vmax=3)\n",
       "axs[0].axis('equal')\n",
       "axs[0].set_title('Approx')\n",
       "axs[1].scatter(*x.T,c=y_exact,vmin=-3,vmax=3)\n",
       "axs[1].axis('equal')\n",
       "axs[1].set_title('Exact')\n",
       "axs[2].scatter(*x.T,c=y_subsample - y_exact,vmin=-1,vmax=1,cmap=plt.cm.seismic)\n",
       "axs[2].axis('equal')\n",
       "axs[2].set_title('Diff')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "93ce558d-596b-4cbf-ab07-0adc6ca46d9d",
      "metadata": {},
      "source": [
       "Unsurprisingly, the quality of the approximation depends heavily on the number of sampled columns - if we choose $d=n$, then we haven't done any compression at all!  **Experiment a bit with different values of $d$ in the above code to get a sense for how the approximation improves for more data.**\n",
       "\n",
       "\n",
       "\n",
       "### Rademacher distributed sample matrix\n",
       "For small $d$, the subsampling described above is obviously extremely lossy: we are just throwing away data!  Perhaps there is a different way to try to do something similar that doesn't waste as much information.  Note that in principle, there shouldn't be any reason why we couldn't use a different choice for the matrix $S$; after all, if we conceptualize the problem as solving a (overdetermined) linear system of equations, then we are multiplying both sides by the same thing.  For example, let's use a matrix where every entry is given by an independent Rademacher distributed random variable (basically a Bernoulli over -1 and 1, rather than 0 and 1, scaled by the root of $n$.  Just as multiplying both sides by a (non-zero) scalar ought to lead to the same solution, it seems intuitive that multiplying both sides by a matrix should lead to close to the same solution.  This also seems like a good idea because it will include all the data in the sketch (even if in an averaged way).  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de64fb3-fe93-492a-9346-c6448c70af53",
      "metadata": {},
      "outputs": [],
      "source": [
       "class RademacherSketch:\n",
       "    def __init__(self,d):\n",
       "        self.d = d\n",
       "\n",
       "    def sketch(self,A,b=None,seed=None):\n",
       "        if seed is not None:\n",
       "            np.random.seed(seed)\n",
       "        n,m = A.shape\n",
       "        scale = 1./np.sqrt(self.d)\n",
       "        S = scale*(np.random.randint(0,2,(self.d,n))*2-1)\n",
       "        SA = S @ A\n",
       "        if b is not None:\n",
       "            Sb = S @ b\n",
       "            return SA,Sb\n",
       "        else:\n",
       "            return SA\n",
       "\n",
       "sketch_rad = RademacherSketch(d)\n",
       "\n",
       "model.fit(*sketch_rad.sketch(Phi,b=y,seed=0))\n",
       "y_rad = model.predict(Phi)\n",
       "\n",
       "fig,axs = plt.subplots(ncols=3)\n",
       "fig.set_size_inches(12,4)\n",
       "axs[0].scatter(*x.T,c=y_rad,vmin=-3,vmax=3)\n",
       "axs[0].axis('equal')\n",
       "axs[0].set_title('Approx')\n",
       "axs[1].scatter(*x.T,c=y_exact,vmin=-3,vmax=3)\n",
       "axs[1].axis('equal')\n",
       "axs[1].set_title('Exact')\n",
       "axs[2].scatter(*x.T,c=y_rad - y_exact,vmin=-1,vmax=1,cmap=plt.cm.seismic)\n",
       "axs[2].axis('equal')\n",
       "axs[2].set_title('Diff')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "1b5c0109-c850-4f2f-b6ef-f2a5c8c33b50",
      "metadata": {},
      "source": [
       "Here we've used the same value for $d$ and the results look okay, but let's get the average error over a number of different trials to compare whether this method actually is better"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0062b87-967c-4b37-a7bc-e4424b8b8f93",
      "metadata": {},
      "outputs": [],
      "source": [
       "n_trials = 200\n",
       "\n",
       "subsample_errors = []\n",
       "for j in range(n_trials):\n",
       "\n",
       "    model.fit(*sketch_subsample.sketch(Phi,b=y))\n",
       "    error = np.sqrt(((model.predict(Phi) - y_exact)**2).mean())\n",
       "    subsample_errors.append(error)\n",
       "\n",
       "rad_errors = []\n",
       "for j in range(n_trials):\n",
       "\n",
       "    model.fit(*sketch_rad.sketch(Phi,b=y))\n",
       "    error = np.sqrt(((model.predict(Phi) - y_exact)**2).mean())\n",
       "    rad_errors.append(error)\n",
       "\n",
       "print(np.mean(subsample_errors),np.std(subsample_errors))\n",
       "print(np.mean(rad_errors),np.std(rad_errors))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "451afe4e-e0ad-4f1f-a9fd-d9f23576b761",
      "metadata": {},
      "source": [
       "We see that the errors in doing this are smaller, and there is greater run-to-run consistency.  Unfortunately, this method doesn't actually lead to much of a benefit.  The reason for this is that the matrix product $S \\Phi$ is expensive to compute.  However, maybe we can take a middle ground approach that gets better accuracy than just randomly subsampling, while also allowing a fast matrix product.\n",
       "\n",
       "## Better random matrix transforms\n",
       "There are two principal ways that people have sought to efficiently produce sketches.  The first is conceptually easy: take the Rademacher approach as described above, but limit each column to contain only $\\zeta$ non-zero entries.  As such the matrix will be sparse, and we can use matrix multiplication routines that capitalize on that sparsity to reduce the expense to $\\mathcal{O}(\\zeta m n)$.  This sketching technique - which is called the sparse sign embedding - looks something like this:\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1cbbfcd-747a-4b5b-876c-40f33328f36e",
      "metadata": {},
      "outputs": [],
      "source": [
       "from scipy.sparse import lil_array\n",
       "\n",
       "class SparseSignSketch:\n",
       "    def __init__(self,d,zeta):\n",
       "        self.d = d\n",
       "        self.zeta = zeta\n",
       "\n",
       "    def sketch(self,A,b=None,seed=None):\n",
       "        if seed is not None:\n",
       "            np.random.seed(seed)\n",
       "        n,m = A.shape\n",
       "        S = lil_array((self.d,n))\n",
       "        scale = 1./np.sqrt(self.zeta)\n",
       "        for i in range(n):\n",
       "            rows = np.random.choice(self.d,self.zeta,replace=False)\n",
       "            S[rows,i] = scale*(np.random.randint(0,2,self.zeta)*2 - 1)\n",
       "\n",
       "        SA = S @ A\n",
       "        if b is not None:\n",
       "            Sb = S @ b\n",
       "            return SA,Sb\n",
       "        else:\n",
       "            return SA\n",
       "\n",
       "zeta = 8\n",
       "sketch_sparse = SparseSignSketch(d,zeta)\n",
       "\n",
       "model.fit(*sketch_sparse.sketch(Phi,b=y,seed=0))\n",
       "y_sparse = model.predict(Phi)\n",
       "\n",
       "fig,axs = plt.subplots(ncols=3)\n",
       "fig.set_size_inches(12,4)\n",
       "axs[0].scatter(*x.T,c=y_sparse,vmin=-3,vmax=3)\n",
       "axs[0].axis('equal')\n",
       "axs[0].set_title('Approx')\n",
       "axs[1].scatter(*x.T,c=y_exact,vmin=-3,vmax=3)\n",
       "axs[1].axis('equal')\n",
       "axs[1].set_title('Exact')\n",
       "axs[2].scatter(*x.T,c=y_sparse - y_exact,vmin=-1,vmax=1,cmap=plt.cm.seismic)\n",
       "axs[2].axis('equal')\n",
       "axs[2].set_title('Diff')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "35206c41-3819-4f95-b956-466d4089d9be",
      "metadata": {},
      "outputs": [],
      "source": [
       "sparse_errors = []\n",
       "for j in range(n_trials):\n",
       "\n",
       "    model.fit(*sketch_sparse.sketch(Phi,b=y))\n",
       "    error = np.sqrt(((model.predict(Phi) - y_exact)**2).mean())\n",
       "    sparse_errors.append(error)\n",
       "\n",
       "print(np.mean(subsample_errors),np.std(subsample_errors))\n",
       "print(np.mean(rad_errors),np.std(rad_errors))\n",
       "print(np.mean(sparse_errors),np.std(sparse_errors))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a28d05da-d30c-4035-8bce-1cbcdf1d1333",
      "metadata": {},
      "source": [
       "We end up with results that don't differ much from the full Rademacher embedding at a substantially lower (asymptotic) cost.  This is, in general, a pretty good method to use by default.  \n",
       "\n",
       "The other common approach is somewhat more esoteric.  The idea is - in essence - to change the basis of the matrix and perform subsampling in the other basis.  Why is this a good idea?  Consider subsampling as above.  The principle weakeness of this method comes from the fact that we might accidentally sample unimportant points (like two that are right next to one another) or we might miss points that exert a strong control on the data.  The formal metric for deciding the \"importance\" of a point is called leverage, and sensible greedy algorithms exist for subsampling by leverage score.  Unfortunately, computing leverage is expensive.  The alternative approach is to instead transform the data matrix into a basis that is less local and more global, such as the Fourier or Walsh-Hadamard space, and to perform subsampling there.  Since these basis functions are global (rather than pointwise), their leverage scores tend to be uniformly distributed.  The expense then comes from the change of basis.  However, fast algorithms for changing from the standard to Fourier basis (or other alternatives) exist - these are called Fast Fourier Transforms, and they can perform the basis change in $n \\log n$ time.  As such, we have the matrix \n",
       "$$\n",
       "S = R F D,\n",
       "$$\n",
       "where $R$ is a subsampling matrix, $F$ is a structured matrix allows for fast application, and $D$ is a diagonal matrix that effects a random sign flip.  Here we'll use the Fast Walsh Hadamard Transform."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45fa098-aef4-4a3d-ad22-07e82f160ab4",
      "metadata": {},
      "outputs": [],
      "source": [
       "def fwht(x):\n",
       "    \"\"\"\n",
       "    Fast Walsh-Hadamard Transform (FWHT) - recursive implementation.\n",
       "\n",
       "    Args:\n",
       "        x (numpy.ndarray): Input array (length must be a power of 2).\n",
       "\n",
       "    Returns:\n",
       "        numpy.ndarray: Transformed array.\n",
       "    \"\"\"\n",
       "    N = len(x)\n",
       "    if N <= 1:\n",
       "        return x\n",
       "    else:\n",
       "        even = fwht(x[0::2])\n",
       "        odd = fwht(x[1::2])\n",
       "        return np.concatenate([even + odd, even - odd])\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dbde967-18d1-4f44-a3b0-f90777618915",
      "metadata": {},
      "outputs": [],
      "source": [
       "class HadamardSketch:\n",
       "    def __init__(self,d):\n",
       "        self.d = d\n",
       "\n",
       "    def sketch(self,A,b=None,seed=None):\n",
       "        if seed is not None:\n",
       "            np.random.seed(seed)\n",
       "        n,m = A.shape\n",
       "        indices = np.random.choice(n,self.d,replace=False)\n",
       "        D = np.random.randint(0,2,n)*2 - 1\n",
       "        scale = 1./np.sqrt(d)\n",
       "        SA = scale*fwht(D.reshape(-1,1)*A)[indices]\n",
       "        if b is not None:\n",
       "            Sb = scale*fwht(D.reshape(-1,1)*b.reshape(-1,1))[indices].ravel()\n",
       "            return SA,Sb\n",
       "        else:\n",
       "            return SA\n",
       "\n",
       "d = 200\n",
       "\n",
       "sketch_hadamard = HadamardSketch(d)\n",
       "\n",
       "model.fit(*sketch_hadamard.sketch(Phi,b=y,seed=0))\n",
       "y_hadamard = model.predict(Phi)\n",
       "\n",
       "fig,axs = plt.subplots(ncols=3)\n",
       "fig.set_size_inches(12,4)\n",
       "axs[0].scatter(*x.T,c=y_hadamard,vmin=-3,vmax=3)\n",
       "axs[0].axis('equal')\n",
       "axs[0].set_title('Approx')\n",
       "axs[1].scatter(*x.T,c=y_exact,vmin=-3,vmax=3)\n",
       "axs[1].axis('equal')\n",
       "axs[1].set_title('Exact')\n",
       "axs[2].scatter(*x.T,c=y_hadamard - y_exact,vmin=-1,vmax=1,cmap=plt.cm.seismic)\n",
       "axs[2].axis('equal')\n",
       "axs[2].set_title('Diff')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1e2301-9f9f-4cb9-8c50-6dfac23eed99",
      "metadata": {},
      "outputs": [],
      "source": [
       "print(np.linalg.norm(sketch_hadamard.sketch(Phi),axis=0))\n",
       "np.linalg.norm(Phi,axis=0)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "98776f7e-3c38-48a0-abf2-6bb4715bba85",
      "metadata": {},
      "outputs": [],
      "source": [
       "hadamard_errors = []\n",
       "ws = []\n",
       "for j in range(n_trials):\n",
       "\n",
       "    model.fit(*sketch_hadamard.sketch(Phi,b=y))\n",
       "    error = np.sqrt(((model.predict(Phi) - y_exact)**2).mean())\n",
       "    hadamard_errors.append(error)\n",
       "    ws.append(model.w)\n",
       "\n",
       "print(np.mean(subsample_errors),np.std(subsample_errors))\n",
       "print(np.mean(rad_errors),np.std(rad_errors))\n",
       "print(np.mean(sparse_errors),np.std(sparse_errors))\n",
       "print(np.mean(hadamard_errors),np.std(hadamard_errors))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f9a3f050-ee0d-455d-a147-48e9e9091880",
      "metadata": {},
      "source": [
       "### Why does this work?\n",
       "While it seems intuitive that this *might* work (at least to me), it's not really clear why it *does* work.  To see why this is the case, we need to get an understanding for what the matrix product changes, and what it leaves the same.  The essential property of all of the matrices that we've considered is the following property called the *subspace embedding property*\n",
       "$$\n",
       "\\| S A x \\| \\le (1 + \\epsilon) \\| A x \\|\\; \\forall x\\in\\mathbb{R}^m.\n",
       "$$\n",
       "Stated in words, the length of any vector $Ax$ remains the same (up to a distortion factor $\\epsilon$) when you multiply it by $S$, even though the dimensionality of that vector is usually not the same.  In fact, theory can provide us a (relatively conservative) bound on the number of sketching elements required to achieve a given distortion factor (approximately $d = \\frac{\\log n}{\\epsilon^2}$).  Let's see what we mean by this:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "86b2d8a1-11cf-4228-bb0b-5a086e124a96",
      "metadata": {},
      "outputs": [],
      "source": [
       "eps = 0.1\n",
       "m = Phi.shape[1]\n",
       "d = int(np.log(n_data)//eps**2)\n",
       "\n",
       "sketch = HadamardSketch(d)\n",
       "\n",
       "SA,Sb = sketch.sketch(Phi,y)\n",
       "\n",
       "v = np.random.randn(m,500)\n",
       "\n",
       "G1 = SA @ v\n",
       "G2 = Phi @ v\n",
       "\n",
       "plt.figure()\n",
       "plt.plot(np.linalg.norm(G2,axis=0),np.linalg.norm(G1,axis=0),'k.')\n",
       "plt.plot(np.linalg.norm(G2,axis=0),np.linalg.norm(G2,axis=0)*(1+eps))\n",
       "plt.plot(np.linalg.norm(G2,axis=0),np.linalg.norm(G2,axis=0)*(1-eps))   "
      ]
     },
     {
      "cell_type": "markdown",
      "id": "5eafd62c-076e-418f-ae9c-b367dfba123c",
      "metadata": {},
      "source": [
       "The norms are preserved within the predicted bounds!  How does this interact with linear regression?  Consider the form of the normal equations\n",
       "$$\n",
       "(\\Phi^T \\Phi + \\gamma \\sigma^2 I) \\mathbf{w} = \\Phi^T y.\n",
       "$$\n",
       "(Note that I've factored out the (diagonal and uniform) observational uncertainty to make the algebra more clear).  Let's look at the product $\\Phi^T \\Phi$.  Its $k$-th diagonal entry is \n",
       "$$\n",
       "\\phi_k^T \\phi_k = \\|\\phi_k\\|_2^2.\n",
       "$$\n",
       "If norms are preserved, then we can immediately use the fact that the relative difference in norms between the sketched matrix and the original to show that the diagonals in the normal equation are approximately the same.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5261013-da12-42a7-9127-ed436d381d01",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.plot(np.diag(Phi.T @ Phi),np.diag(SA.T @ SA),'k.')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d3d59179-87bc-47c7-8a8d-2d1234a73aa6",
      "metadata": {},
      "source": [
       "Perhaps surprisingly, subspace embeddings also preserve angles.  This (in conjunction with the preservation of norms) implies that *dot products* are generally preserved (of which the norm is the special case of the dot product of a vector with itself).  As such, we would expect all of the entries in $\\Phi^T \\Phi$ (which are just dot products of rows with columns) to be reasonably close to $(S \\Phi)^T (S \\Phi)$ (and the same for $\\Phi^T y$ versus $(S \\Phi)^T Sy$)."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2f910d-1322-44f6-96c8-de230c23d5cb",
      "metadata": {},
      "outputs": [],
      "source": [
       "fig,axs = plt.subplots(ncols=3)\n",
       "fig.set_size_inches(12,4)\n",
       "axs[0].imshow(Phi.T @ Phi,vmin=0,vmax=10)\n",
       "axs[1].imshow(SA.T @ SA,vmin=0,vmax=10)\n",
       "axs[2].plot(Phi.T @ y,SA.T @ Sb,'k.')\n",
       "axs[2].plot(np.array([-100,100]),np.array([-100,100]))\n",
       "axs[2].axis('equal')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "c9ce52cc-aa06-4320-ae63-e4517ceeea87",
      "metadata": {},
      "source": [
       "We can manipulate these properties to also gain a similar bound on errors in predictions - when we make a prediction using coefficients $\\mathbf{w}$ derived by solving the sketched least squares problem, the resulting cost is bounded by $1+\\epsilon$ times the minimum of the full problem.  However, we do *not* have a strong guarantee on whether the actual values of $\\mathbf{w}$ determined from the sketched problem converge to the solution of the unsketched problem.  Indeed, if we solve the sketched problem many times:\n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "8645c292-340a-4b78-a020-8820dc2a8ad6",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.plot(w_exact,np.column_stack(ws),'.')\n",
       "plt.axis('equal')\n",
       "plt.xlabel('Exact w')\n",
       "plt.ylabel('Approximate w')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d5c1f9ab-8bd8-4a2b-8b11-7353da332bd6",
      "metadata": {},
      "source": [
       "However, there are alternative methods that allow for this error to be substantially reduced, if that is a requirement for the application.\n",
       "\n",
       "## Column-space embeddings\n",
       "In the previous example, we had a large number of data points that we wanted to reduce to a smaller number of \"aggregate\" points for computational efficiency when solving a linear regression problem.  In solving the normal equations, we saw that geometry was preserved over the reduced rows, the normal equations used to solve the least squares problem was only mildly perturbed.  However, for some problems, we might not be so interested in fitting a curve as we are in categorizing data instances - for example in clustering or logistic regression.  In these cases, it doesn't necessarily make sense to try to aggregate data points - but it might make sense to try to reduce the dimensionality of the feature space, i.e. to covert a data matrix $X\\in \\mathbb{R}^{n\\times m}$ into $\\hat{X}\\in \\mathbb{R}^{n\\times d}$.  Perhaps unsurprisingly, we  can do a largely similar thing, but applied to the data matrix's column space instead of its row space, i.e. performing right multiplication instead.  \n",
       "\n",
       "Let's do clustering first. Let's try applying this to the MNIST digits dataset."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d4a8ff-f1d7-48bb-849f-b01e7b0b9daf",
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "from sklearn.datasets import fetch_openml\n",
       "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
       "X = mnist.data/255.\n",
       "plt.imshow(X[0].reshape((28,28)))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "85a53710-ffbf-4514-ba49-6f5fc2ff0fae",
      "metadata": {},
      "source": [
       "We can generate a much compressed version of the mnist dataset for $k-$means clustering by simply right multiplying by a similar S matrix to that derived above"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "70913e89-9839-460e-9159-870737cdae55",
      "metadata": {},
      "outputs": [],
      "source": [
       "d = 200\n",
       "zeta = 8\n",
       "sketch = SparseSignSketch(d,zeta)\n",
       "Xhat = sketch.sketch(X.T).T"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a719d0ce-d1e9-4ea8-acf9-bc581930969e",
      "metadata": {},
      "source": [
       "First, just as a sanity check, we might ask whether we continue to preserve lengths - this time over rows.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef9b8a4-650a-4505-a542-f1af5fe1ffd5",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.plot(np.linalg.norm(X,axis=1),np.linalg.norm(Xhat,axis=1),'k.')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "25fe0d37-dfcb-4a5e-99f4-c515ced681e4",
      "metadata": {},
      "source": [
       "Yes!  This is important because fundamentally, what k-means is doing is computing distances (in feature space) from any point to a cluster center, and so its important that those distances are preserved.  We will use scikit-learn's built in clustering algorithm, which has effective methods for initialization, etc. - no need to reinvent the wheel here.  We will compare the clustering algorithm's behavior on both the full and reduced dataset.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22b525b-d273-41d0-a6b1-8f73c8f56238",
      "metadata": {},
      "outputs": [],
      "source": [
       "from sklearn.cluster import KMeans\n",
       "\n",
       "n_clusters = 10\n",
       "\n",
       "models = []\n",
       "for data in [X,Xhat]:\n",
       "    model = KMeans(n_clusters)\n",
       "    model.fit(data)\n",
       "    models.append(model)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "34e8f0b5-31ef-4bf9-8dcb-6fe985ed904b",
      "metadata": {},
      "source": [
       "In general, it's difficult to analyze the performance of an unsupervised classification algorithm.  However, in this case we do have labels.  Assuming that our clustering is reasonably effective at dividing different digits into different groups, we can map from a true label $k$ (which the model has never seen) to the model's arbitrary class by evaluating which cluster has the most $k$'s in it."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9aef4a8-5a1d-4ec8-ae42-4b75523c2e90",
      "metadata": {},
      "outputs": [],
      "source": [
       "from scipy.stats import mode\n",
       "desired_index = 3\n",
       "\n",
       "def sample_cluster_by_desired_digit(X,model,target,index):\n",
       "    clusters = model.predict(X)\n",
       "    cluster_index = mode(clusters[target==index]).mode\n",
       "    return clusters == cluster_index\n",
       "\n",
       "for X_,model in zip([X,Xhat],models):\n",
       "    mask = sample_cluster_by_desired_digit(X_,model,mnist.target.astype(int),desired_index)      \n",
       "    fig,axs = plt.subplots(nrows=4,ncols=4)\n",
       "    for ax in axs.ravel():\n",
       "        i = np.random.randint(mask.sum())\n",
       "        ax.imshow(X[mask][i].reshape(28,28))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "276ecfc3-0d92-4a48-a4b5-de86cd28f5c1",
      "metadata": {},
      "source": [
       "Just for the purposes of quantitative comparison, we can try something similar with logistic regression.  We'll split the data up into a training and test set and compute the test accuracy."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a02263-6f64-4db7-83f3-ad389976ca7e",
      "metadata": {},
      "outputs": [],
      "source": [
       "from sklearn.linear_model import LogisticRegression\n",
       "\n",
       "rand_ind = np.random.permutation(range(70000))\n",
       "n_train = 60000\n",
       "\n",
       "training_indices = rand_ind[:n_train]\n",
       "test_indices = rand_ind[n_train:]\n",
       "\n",
       "target = mnist.target.astype(int)\n",
       "\n",
       "models = [LogisticRegression(tol=1e-3) for _ in range(2)]\n",
       "\n",
       "accuracy = []\n",
       "\n",
       "for X_,model in zip([X,Xhat],models):\n",
       "    model.fit(X_[training_indices],target[training_indices])\n",
       "    accuracy.append(np.mean(model.predict(X_[test_indices]) == target[test_indices]))\n",
       "    \n",
       "print(accuracy)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "bc6714f8-def2-419a-aabd-51e509c9e6a7",
      "metadata": {},
      "source": [
       "Despite having reduced our data by 1/7th via exclusively random projection, we have retained most of our test accuracy!  \n",
       "\n",
       "### Randomized PCA\n",
       "As a final example of randomized methods, we will explore an efficient mechanism to compute an approximate Principal Component Analysis of a (perhaps large) dataset.  This methodology is slightly more in depth (and more accurate) than the previous two.  For the PCA, it also relies upon the fact that a data set that lives in $\\mathbb{R}^{n \\times m}$ might actually lie on something close to a manifold, which is to say an embedded geometric structure of dimension $r$ where $r<m$ (for example a line in a 2d plane, or a line or plane in a 3D volume, etc).  PCA reveals this structure ex post facto - but if we can assume it a priori, then we can accelerate computation.  \n",
       "\n",
       "### Low rankedness\n",
       "Before we get back to the PCA, let's think a little bit about what a matrix does, and some structures that it might have.  First, let's consider some vectors.  Let's begin by generating and plotting a couple of random vectors."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "333927e6-ba16-43a9-af00-f851002894ab",
      "metadata": {},
      "outputs": [],
      "source": [
       "def vec_plot(y,c=None,ls='solid'):\n",
       "    #if c is None:\n",
       "        #c = 'black'\n",
       "    origin = np.column_stack([[0, 0]]*y.shape[1])\n",
       "    plt.plot(0,0,'ok')\n",
       "    if c is not None:\n",
       "        plt.quiver(*origin,*y,c,angles='xy', scale_units='xy', scale=1, ls=ls, facecolor='none', linewidth=1)\n",
       "    else:\n",
       "        plt.quiver(*origin,*y,angles='xy', scale_units='xy', scale=1, ls=ls, facecolor='none', linewidth=1)\n",
       "    plt.axis('equal')\n",
       "    plt.axhline(y=0, color='k')\n",
       "    plt.axvline(x=0, color='k')\n",
       "    plt.xlim(-3,3)\n",
       "    plt.ylim(-3,3)\n",
       "    \n",
       "x = np.random.rand(2,3) * 2 - 1\n",
       "c = np.linspace(0,1,x.shape[1])\n",
       "vec_plot(x,c)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "52909969-c666-42c8-a415-c7b89af4bdae",
      "metadata": {},
      "source": [
       "Now let's multiply these same vectors, and see what happens to them."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "386a110e-d427-40e4-b3a7-a9e77f7ac3f9",
      "metadata": {},
      "outputs": [],
      "source": [
       "A_full = np.random.randn(2,2)\n",
       "vec_plot(A_full @ x, c,ls='dashed')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f9d7d620-ab7f-484c-ac7e-d29e8be6d6dc",
      "metadata": {},
      "source": [
       "Obviously they get transformed - but how exactly?  Let's look at the matrix that did the multiplying.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f3f149-4d34-4561-87cf-ee49bda48d0a",
      "metadata": {},
      "outputs": [],
      "source": [
       "print(A_full)\n",
       "print(x[:,0])\n",
       "print(A_full[:,0]*x[0,0] + A_full[:,1]*x[1,0])"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "2c3ae7b4-b64c-4af5-afe3-e6585c799e94",
      "metadata": {},
      "source": [
       "The matrix multiplication formula for $A\\mathbf{x}$ can be written as $a_1 x_1 + a_2 x_2$, where $a_i$ is the $i$-th column of $A$.  As such matrix multiplication is saying take $x_1$ of the first column of $A$ plus $x_2$ of the second column of $A$ and so on.  As such, one way of looking at a matrix is as a *basis* for some space (e.g. $\\mathbb{R}^2$ or something else) - and the space is defined as the span of the columns of $A$ (the set of vectors produced by taking all possible linear combinations of the basis).  The act of matrix multiplication takes the coordinates of a point defined in terms of the basis $A$ and turns it into a basis defined in terms of the standard basis, which has a special matrix associated with it\n",
       "$$\n",
       "I = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}.\n",
       "$$\n",
       "This makes sense - if a vector that is represented via the standard basis is transformed to be represented in the standard basis, then nothing happens.  If a vector representing as the coefficients of a different basis are multiplied by a matrix, then we recover the representation of that vector in the standard basis.  Here's another example: let's say that we have as our basis."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf1cfe33-2285-4c66-b832-945d1d30ce52",
      "metadata": {},
      "outputs": [],
      "source": [
       "A = np.array([[np.sqrt(2)/2,-np.sqrt(2)/2],[np.sqrt(2)/2,np.sqrt(2)/2.]])\n",
       "vec_plot(A)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "822d05df-9488-4507-b71f-f53abcb40b3f",
      "metadata": {},
      "source": [
       "If we multiply this matrix by the vector $[1,1]^T$, what we are asking for is the first basis vector plus the second basis vector represented in the standard basis.  This ought to be aligned with the vertical axis, and because the basis vectors are orthogonal and unit length, the norm of the vector ought not to change.   "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f245264e-88d2-429e-b241-cb054830e086",
      "metadata": {},
      "outputs": [],
      "source": [
       "x = A @ np.array([[1],[1]])\n",
       "\n",
       "vec_plot(x)\n",
       "print(x)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7253f2ea-d8ca-4d1b-83d5-8e32a58a37ff",
      "metadata": {},
      "source": [
       "What is the space for which $A$ is the basis?  Well, it should be pretty obvious that we can represent any vector in $\\mathbb{R}^2$ as a linear combination of either the standard basis (columns of the identity) or the columns of $A$.  In fact, for most matrices in $\\mathbb{R}^2$, this is the case. "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7b253e-6776-43b1-9c35-6f5220e7d78b",
      "metadata": {},
      "outputs": [],
      "source": [
       "x_rand = np.random.randn(2,100)\n",
       "vec_plot(x_rand)\n",
       "plt.figure()\n",
       "vec_plot(A_full @ np.random.randn(2,100))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f3770f55-8c9a-4266-92de-891ed2021481",
      "metadata": {},
      "source": [
       "However, there are examples for which this is *not* the case.  For example, consider the following matrix, which we can also try multiplying by a bunch of random vectors"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5529677-b5de-4d9d-801b-c964a63d68fe",
      "metadata": {},
      "outputs": [],
      "source": [
       "A_bad = np.random.randn(2,1) @ np.random.randn(1,2)\n",
       "\n",
       "vec_plot(x_rand)\n",
       "plt.figure()\n",
       "vec_plot(A_bad @ x_rand)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "b3ee7879-b5be-4617-a6cf-18f7dce5084f",
      "metadata": {},
      "source": [
       "All these vectors are being mapped to a line!  Despite having two basis vectors that we could add up in different combinations to produce output vectors, adding those vectors always produced output vectors that would lie on a line.  Stated another way, the space spanned by the columns was of a lower dimension than the number of columns (you should convince yourself that this only happens when one column is a constant multiple of another).  We call a matrix for which this is the case \"low rank\".\n",
       "\n",
       "For lots of applications, this is viewed as a negative property.  For example, such matrices are not invertible.  However, it also offers a path towards compression.  \n",
       "\n",
       "To do this, let's first try to understand the line to which all of our vectors are being mapped.  These vectors are still in $\\mathbb{R}^2$, but it's clear that they are uniquely indexed by some constant multiple of some vector $\\mathbf{q}$ that forms a basis for the 1D subspace.  From that perspective, the coordinate of any datapoint is just a scalar, and these can be mapped back to $\\mathbb{R}^2$ by multiplying by $\\mathbf{q}$.  How do we get $\\mathbf{q}$?  Well, we already have a bunch of copies of it scaled by different factors!  The scaling is non-unique, so let's choose to take one of unit length."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf3c2d0-a848-477d-beff-51324ec79094",
      "metadata": {},
      "outputs": [],
      "source": [
       "y_bad = A_bad @ x_rand\n",
       "q = (y_bad[:,0]/np.linalg.norm(y_bad[:,0])).reshape(-1,1)\n",
       "vec_plot(A_bad @ x_rand)\n",
       "plt.plot(*(q*np.array([-3,3])),'r-')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "deae6dbd-7af4-471f-a4bb-160dd6a698e8",
      "metadata": {},
      "source": [
       "Clearly, this spans the space.  Now, to represent the dataset of vectors using only a single number, we need to know where - in terms of the standard basis on the 1-D subspace - the vectors $\\mathbf{x}$ map to.  In fact this is is precisely what $\\mathbf{q}^T$ does: take something represented in a particular 2D basis, and represent it in the standard basis in 1D.  As such, it ought to be the cases that\n",
       "$$\n",
       "A\\mathbf{x} = \\mathbf{q} \\mathbf{q}^T A \\mathbf{x} \\; \\forall \\mathbf{x}.\n",
       "$$"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c573b5-4a9f-4988-9ac2-969a8367d7fc",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.plot(q @ (q.T @ A_bad @ x_rand),A_bad @ x_rand,'k.')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f8ecbfd6-f85b-48b5-aed4-1bb6f693e7c1",
      "metadata": {},
      "source": [
       "This is lossless compression.  Now, instead of representing a dataset element as $\\mathbf{y} = A \\mathbf{x}$, we could represent it as $c=\\mathbf{q}^T A \\mathbf{x}$, which is a scalar for each datapoint.  When we need to recover $\\mathbf{x}$, we can multiply it by $\\mathbf{q}$. \n",
       "Note that since this holds for all $x$, this result also implies that\n",
       "$$\n",
       "A = \\mathbf{q} (\\mathbf{q}^T A).\n",
       "$$\n",
       "\n",
       "While we have applied this approach to a very simple case of a 1-D manifold immersed in a 2-D space, the same ideas generally hold for a $k$-dimensional manifold immersed in a $m$-dimensional space.  We can't visualize high dimensional spaces, but we can see this same structure emerge numerically.  The trick is in figuring out how to compute the basis for the manifold $\\mathbf{q}$.  However, this is where randomization comes in - if we take the product $A S^T$, with $S^T$ defined as one of the random matrices described previously, then with high probability\n",
       "$$\n",
       "Y = A S^T\n",
       "$$\n",
       "spans the same space as $A$.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "513c3201-4b27-44ea-98db-129e6c80f4a4",
      "metadata": {},
      "outputs": [],
      "source": [
       "k = 20\n",
       "m = 50\n",
       "\n",
       "A = np.random.randn(m,k)/np.sqrt(k) @ np.random.randn(k,m)/np.sqrt(k)\n",
       "\n",
       "sketch = SparseSignSketch(k,8)\n",
       "Y = sketch.sketch(A.T).T"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "dfa2c0a8-b57d-4f36-9e44-74f9ca54ef97",
      "metadata": {},
      "source": [
       "Unfortunately, we can't use $Y$ as $q$ directly because the columns aren't orthogonal or unit length (and so $q^T$ doesn't map to the standard basis on the manifold).  However, we can produce an orthogonal and normal basis (so-called \"orthonormal\") by applying a simple procedure called Gram-Schmidt Orthogonalization.  We will not delve into the numerics of this process - that is better suited for a numerical linear algebra course - but there are two important facts.  First, this procedure is available to us by calling "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "0534ce75-8989-4613-9145-b581efec247a",
      "metadata": {},
      "outputs": [],
      "source": [
       "q,_ = np.linalg.qr(Y)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7e6569dd-bda9-4a41-9d7d-6bb8869ba02e",
      "metadata": {},
      "source": [
       "and this function takes $\\mathcal{O}(mk^2)$ time.  Did this work?"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d81bac17-4b22-4a41-b428-9e053f55958f",
      "metadata": {},
      "outputs": [],
      "source": [
       "A"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "1904c403-ed61-4a9d-99ae-94e9553bcfaa",
      "metadata": {},
      "outputs": [],
      "source": [
       "c = q.T @ A\n",
       "print(c.shape)\n",
       "print(q @ c)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f9fb7167-ebbf-4a16-864f-aeac2fcae9b2",
      "metadata": {},
      "source": [
       "One final note: this process still works to create a reasonable approximation even when the manifold is only approximately a manifold - like when you have a lot of variance in one direction and only a little bit of variance in the other direction, but both are non-zero."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "099e8d0c-87de-40ce-ad7e-b0ae5af92690",
      "metadata": {},
      "outputs": [],
      "source": [
       "L_bad = np.random.randn(2,2)\n",
       "V,_ = np.linalg.qr(L_bad)\n",
       "A_bad = V * np.array([1,0.25]) @ V.T\n",
       "\n",
       "vec_plot(x_rand)\n",
       "plt.figure()\n",
       "vec_plot(A_bad @ x_rand)\n",
       "\n",
       "x_r = np.random.randn(2,1)\n",
       "\n",
       "q,_ = np.linalg.qr(A_bad @ x_r)\n",
       "#q,_,_ = np.linalg.svd(A_bad @ x_r)\n",
       "plt.plot(*q[:,0][:,np.newaxis]*np.array([-3,3]))\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "978367ba-2080-4c95-b933-a968f835c897",
      "metadata": {},
      "source": [
       "Note that its awkward to illustrate this with a space that's low dimensional enough to be plotted because it only takes two samples for any $\\mathbf{q}$ to span the whole space, so there's not much utility here.\n",
       "\n",
       "### Principal Component Analysis\n",
       "Let's recall how PCA works.  Let's consider that we have a data matrix, which we'll call $X$.  For this example, we could either use MNIST again, or we could use the classic example of \"eigenfaces\" - which is the application of PCA to datasets of facial images.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "550aaf54-b7b5-4d30-bd50-ad7af819b288",
      "metadata": {},
      "outputs": [],
      "source": [
       "from sklearn.datasets import fetch_lfw_people\n",
       "lfw_people = fetch_lfw_people(min_faces_per_person=1, resize=0.4)\n",
       "plt.imshow(lfw_people.data[0].reshape(50,37),cmap=plt.cm.grey)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "70146012-87c5-4146-b3da-2a3d18766552",
      "metadata": {},
      "source": [
       "This is a pretty big dataset.  Before we apply PCA there, let's recall the intuition.  Let's say that we've got some dataset:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "da06d772-824b-481b-9d2f-dc37ae2f3f91",
      "metadata": {},
      "outputs": [],
      "source": [
       "np.random.seed(0)\n",
       "L = np.random.randn(2,2)\n",
       "V,_ = np.linalg.qr(L)\n",
       "A_corr = V * np.array([1,0.1]) @ V.T\n",
       "X = (A_corr @ np.random.randn(2,1000)).T + np.array([2,1])\n",
       "plt.scatter(*X.T)\n",
       "plt.axhline(y=0, color='k')\n",
       "plt.axvline(x=0, color='k')\n",
       "plt.axis('equal')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "14cde263-92fe-4aa9-8598-de3ea58d862d",
      "metadata": {},
      "source": [
       "First, for the sake of simplicity, let's subtract the mean so that we're centered on the origin."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "89143116-1ec5-4b1a-962a-5f3c630402ce",
      "metadata": {},
      "outputs": [],
      "source": [
       "mu = X.mean(axis=0)\n",
       "plt.scatter(*(X - mu).T)\n",
       "plt.axhline(y=0, color='k')\n",
       "plt.axvline(x=0, color='k')\n",
       "plt.axis('equal')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "09565b06-5688-4067-abfd-4c1c2188314c",
      "metadata": {},
      "source": [
       "In order to understand the spread in the data due to different factors, we would like to compute variances (or standard deviations).  To that end, we would like to change our basis so that the two dimensions independent from one another, i.e. \n",
       "$$\n",
       "Z = X V,\n",
       "$$\n",
       "where $V$ is a rotation matrix (i.e. perpendicular and unit-length columns), such that the axes line up with the directions of maximum variability in the data.  In this basis, we can compute a diagonal variance matrix as \n",
       "$$\n",
       "n \\Lambda = Z^T Z = V^T X^T X V. \n",
       "$$\n",
       "Because $V$ is a rotation matrix, $V^T$ is its inverse and so we seek orthonormal $V$ and diagonal $\\Lambda$ such that\n",
       "$$\n",
       "V \\Lambda V^T = \\frac{X^T X}{n}. \n",
       "$$\n",
       "We recognize $\\frac{X^T X}{n}$ as the covariance matrix (assuming the data is centered, otherwise it would be $\\frac{(X - E[X])^T(X-E[X])}{n}$), and the left side as an eigendecomposition - the eigenvectors are the ones that the covariance matrix doesn't rotate, and the eigenvalues are the variances.  The resulting covariance matrix is of size $m\\times m$ and is symmetric (and positive definite, if that means anything to you).  We can compute its eigendecomposition in $\\mathcal{O}(m^3)$ time (but with quite a significant leading order coefficient typically around 40).  Computing the covariance matrix takes $\\mathcal{O}(n m^2)$ time (not unlike the least-squares example).   "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "afaf943d-9f57-412f-8f19-57556752bc77",
      "metadata": {},
      "outputs": [],
      "source": [
       "Lamda, V = np.linalg.eigh(((X-mu).T @ (X-mu))/X.shape[0])\n",
       "mu = X.mean(axis=0)\n",
       "plt.scatter(*(X - mu).T)\n",
       "vec_plot(V*3*np.sqrt(Lamda))\n",
       "\n",
       "plt.axhline(y=0, color='k')\n",
       "plt.axvline(x=0, color='k')\n",
       "plt.axis('equal')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "5701b8df-6867-46c8-8fca-976950474107",
      "metadata": {},
      "source": [
       "This quite helpfully determines the axes along which the data varies independently (the principal components), and the size of the eigenvalues tells us which of these axes has the most variability.\n",
       "\n",
       "In principle, we could do *exactly* the same procedure for the facial dataset.  However, in this case it has  $m=1850$ features and $n=13500$ data points.  As such both forming the covariance matrix *and* computing the eigendecomposition is pretty expensive for a large dataset such as this one.  Instead, we will use randomized methods to compute the first - say - 100 principal components, operating under the assumption that the data is indeed low rank - or that a judicious combination of the columns of the data matrix contain nearly as much information as the full dataset.  \n",
       "\n",
       "The first order of business is to compute the covariance matrix.  We will use the same sketching approach as we did in linear regression to do this efficiently.  Here, we'll use a variant of the Hadamard sketch called the Discrete Cosine Transform Sketch, just because I was bored and wanted to see if it would work.    "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a1af20-474a-4c06-8581-9f7617515b9f",
      "metadata": {},
      "outputs": [],
      "source": [
       "from scipy.fft import dct\n",
       "class DiscreteCosineSketch:\n",
       "    def __init__(self,d):\n",
       "        self.d = d\n",
       "\n",
       "    def sketch(self,A,b=None,seed=None):\n",
       "        if seed is not None:\n",
       "            np.random.seed(seed)\n",
       "        n,m = A.shape\n",
       "        indices = np.random.choice(n,self.d,replace=False)\n",
       "        D = np.random.randint(0,2,n)*2 - 1\n",
       "        scale = 1./np.sqrt(2*d)\n",
       "        SA = scale*dct(D.reshape(-1,1)*A,axis=0)[indices]\n",
       "        if b is not None:\n",
       "            Sb = scale*dct(D.reshape(-1,1)*b.reshape(-1,1),axis=0)[indices].ravel()\n",
       "            return SA,Sb\n",
       "        else:\n",
       "            return SA"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55432b2-5696-4133-9274-08a253817b8b",
      "metadata": {},
      "outputs": [],
      "source": [
       "X = lfw_people.data - lfw_people.data.mean(axis=0)\n",
       "Xbar = X - X.mean(axis=0)\n",
       "Xbar /= Xbar.std()\n",
       "\n",
       "n,d = Xbar.shape\n",
       "sk = DiscreteCosineSketch(d)\n",
       "Xhat = sk.sketch(Xbar)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "4661a448-95ae-4b91-8c18-3979d2486570",
      "metadata": {},
      "source": [
       "Let's just double check that this thing really does preserve the length of the columns"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "800e1d64-6291-4adc-b1b6-48fc78846d09",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.plot(np.linalg.norm(Xbar,axis=0),np.linalg.norm(Xhat,axis=0),'k.')\n",
       "plt.axis('equal')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "e50e256f-2db1-4c02-858d-3e0907f48b38",
      "metadata": {},
      "source": [
       "Now, we can cheaply form the covariance matrix.  (NOTE: There are different orders in which this could be done - here we sketch the columns of the covariance matrix, but one could also sketch the columns of the already-sketched data matrix that we just produced.)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "15905769-8908-4a11-b582-0debe0c57474",
      "metadata": {},
      "outputs": [],
      "source": [
       "C = Xhat.T @ Xhat / n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "361c4ea0-15ef-4deb-a176-673f63b008d2",
      "metadata": {},
      "source": [
       "Now, let's sketch the columns (or rows, since it's a symmetric matrix)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "53576163-e0c8-42cb-8593-9c61fed3921d",
      "metadata": {},
      "outputs": [],
      "source": [
       "sketch_cols = DiscreteCosineSketch(500)\n",
       "Y = sketch_cols.sketch(C).T"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "aa5eb1da-c28e-45cf-a189-d845151005f7",
      "metadata": {},
      "source": [
       "We now have a sketch of the column space of the covariance matrix, and we assume that this column-space lies on a low-dimensional manifold of size less than 500.  Under that assumption, we can use the same machinery that we derived above and assume that \n",
       "$$\n",
       "C \\approx Q Q^T C,\n",
       "$$\n",
       "where $Q$ is an orthonormal basis for the column space of the covariance matrix $C$.  Indeed, we can go one further - because $C$ is symmetric, its column and row spaces are the same, so\n",
       "$$\n",
       "C \\approx Q Q^T C Q Q^T.\n",
       "$$\n",
       "We can show that this is mostly true empirically.  First, compute $Q$ from the sketch."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ffe72d-409d-417f-aac1-e54795bff20e",
      "metadata": {},
      "outputs": [],
      "source": [
       "Q,_ = np.linalg.qr(Y)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a1307c99-c6f9-4d2a-9e61-d4e860a2629e",
      "metadata": {},
      "source": [
       "For convenience, let's compute the small inner term $Q^T C Q$"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbb8463d-df91-4f39-b4df-245a732c0268",
      "metadata": {},
      "outputs": [],
      "source": [
       "B = Q.T @ C @ Q\n",
       "print(B.shape)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0d2d0098-7769-47a2-ab32-ff0a5968dd49",
      "metadata": {},
      "source": [
       "Now let's plot the covariance against its doubly-sketched approximation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e2862e-0143-4a9f-ad8c-aacabb246264",
      "metadata": {},
      "outputs": [],
      "source": [
       "fig,axs = plt.subplots(ncols=3)\n",
       "fig.set_size_inches(18,12)\n",
       "axs[0].imshow(C,vmin=0,vmax=1)\n",
       "axs[1].imshow(Q @ B @ Q.T,vmin=0,vmax=1)\n",
       "axs[2].imshow(Q @ B @ Q.T - C,vmin=-0.05,vmax=0.05,cmap=plt.cm.seismic)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "5b908fca-1a67-455a-b7f5-985e99c4921b",
      "metadata": {},
      "source": [
       "This is really quite accurate.  Now for a cheap eigendecomposition, which is really quite straightforward.  Instead of taking the eigendecomposition of $C$, we'll take the eigendecomposition of $B$:  \n",
       "$$\n",
       "C \\approx Q B Q^T = Q U \\Lambda U^T Q^T.\n",
       "$$\n",
       "$\\Lambda$ is a diagonal matrix of size $l \\times l$, while $U$ is an orthonormal matrix of size $m \\times l$.  If we define\n",
       "$$\n",
       "V = Q U,\n",
       "$$\n",
       "then $V$ is another orthonormal matrix (a rotation applied to a rotation is still a rotation after all).  We thus have\n",
       "$$\n",
       "C \\approx V \\Lambda V^T,\n",
       "$$\n",
       "with $V$ orthonormal and $\\Lambda$ diagonal, which is *by definition* and eigendecomposition of $C$, and thus the PCA!  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2443df2-9190-4edd-97f9-965da86a31d2",
      "metadata": {},
      "outputs": [],
      "source": [
       "Lamda,U = np.linalg.eigh(B)\n",
       "\n",
       "# eigh produces eigenvalues in ascending rather than the standard descending order - flip em around\n",
       "Lamda = Lamda[::-1]\n",
       "U = U[:,::-1]\n",
       "\n",
       "V = Q @ U"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "7c86221c-950c-4a21-8e86-73d6e8d783a5",
      "metadata": {},
      "source": [
       "Let's look at the first 9 principal components."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "18eb7eb0-1820-43aa-9685-375bacdd59b5",
      "metadata": {},
      "outputs": [],
      "source": [
       "fig,axs = plt.subplots(nrows=3,ncols=3)\n",
       "fig.set_size_inches(12,12)\n",
       "axs = axs.ravel()\n",
       "for i,ax in enumerate(axs):\n",
       "    ax.imshow(V[:,i].reshape(50,37),cmap=plt.cm.gray)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f4f743d5-c668-406f-af9f-bc5842807dfe",
      "metadata": {},
      "source": [
       "How expensive was this to compute?  First we had to sketch the rows, which required $\\mathcal{O}(n \\log n m)$ operations (because of the sketch that we used.  Different embeddings may have been cheaper).  Next we had to compute the covariance matrix, which cost $\\mathcal{O})(d^2 m)$, where $d$ was, in this case quite a bit smaller than $m$.  Next we had to sketch it, which cost $\\mathcal{O}(m^2 \\log m)$.  Next, we had to compute $Q$, which was $\\mathcal{O}(l^2 m)$.  Finally, we computed the eigendecomposition of $B$, which cost $\\mathcal{O}(l^3)$ operations.  Overall, the complexity was $\\mathcal{O}((n\\log n + m \\log m + l^2) m)$.  Compare this to the standard approach, which would have been $\\mathcal{O}((n + m) m^2)$.  This is quite a bit faster asymptotically, especially if we use sketch sizes that are smaller than $m$.  But is the whole thing accurate?  As it turns out, this problem is still not large enough to be prohibitive to compute a \"normal\" PCA.  Let's do that and compare the results."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4739635-45eb-46b0-8ed3-e94b4051ff8a",
      "metadata": {},
      "outputs": [],
      "source": [
       "Lamda_true,V_true = np.linalg.eigh(Xbar.T @ Xbar/n)\n",
       "Lamda_true = Lamda_true[::-1]\n",
       "V_true = V_true[:,::-1]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f450053-1bbb-4cbd-99a1-c014a5969aee",
      "metadata": {},
      "outputs": [],
      "source": [
       "fig,axs = plt.subplots(nrows=3,ncols=3)\n",
       "fig.set_size_inches(12,12)\n",
       "axs = axs.ravel()\n",
       "for i,ax in enumerate(axs):\n",
       "    ax.imshow(np.column_stack((V[:,i].reshape(50,37)*np.sign(V[0,i]),V_true[:,i].reshape(50,37)*np.sign(V_true[0,i]))),cmap=plt.cm.gray)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "55c2b37a-f99a-43ad-9fbb-b6fcb0e55c20",
      "metadata": {},
      "source": [
       "The eigenvectors are very similar (although a bit noisy) - what about the variances?"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c39aa4-c50d-42f2-9545-5274afd22e86",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.semilogy(Lamda_true[:500])\n",
       "plt.semilogy(Lamda[:500])"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "34649f8e-8043-4a8a-920e-25cb23306d47",
      "metadata": {},
      "source": [
       "Note that we could now use this PCA for any number of downstream tasks - for example clustering or classification, for which the PCA-transformed data performs very nearly as well as the original (because PCA is close to lossless compression).  Assuming that a meaningful low dimensional structure exists in column space and the number of non-negligible principal components is actually small, these randomized methods allow for the application of PCA to *enormous* problems with very little accuracy loss."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bc9822d-5e86-4788-a21e-2f9400bd3765",
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }